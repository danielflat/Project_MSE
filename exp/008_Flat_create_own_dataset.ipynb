{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With this notebook, we want to create our own dataset for ranking. The dataset gets saved as a `.xlsx`file in `FILE_PATH`.\n",
    "\n",
    "The idea of this notebook is to create \"expert\" rankings using the crawled data. Since the crawled data contains meta information like \"keywords\" or \"headings\", these meta information can be used to create queries and corresponding rankings of our documents.\n",
    "\n",
    "It works like this:\n",
    "1. We sample a query term based on the given meta information, e.g. the keywords \"tÃ¼bingen neuroscience\" \n",
    "2. We prefilter every document of our index to get a list of true positives. For each query we want to rank `NUMBER_RANKED_DOCUMENTS`, e.g. 100 documents. If there are more than e.g. 100 documents available, we cut of that list. If we have less then 100, we sample some negative samples without replacement to fill up 100 documents.\n",
    "3. We run BM25 to rank these documents\n",
    "4. Because BM25 does not produce the best results and we also cannot fully trust the keywords, we sample gumbel noise for each ranking.\n",
    "5. We calculate a softmax of (BM25 + gumbel noise) for each document and get the final ranking. These ranking be saved as excel.\n",
    "\n",
    "(In the end of the notebook, some are some sanity checks are being done.)\n"
   ],
   "id": "6cda89a6cee23495"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T19:24:47.187373Z",
     "start_time": "2024-07-14T19:24:43.982928Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from ranker.QueryResult import QueryResult\n",
    "from ranker.ranker import RankerFlat\n",
    "from utils.directoryutil import get_path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:24:47.218338Z",
     "start_time": "2024-07-14T19:24:47.189438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CONSTANTS\n",
    "MAX_DATAPOINTS = 100000  # determines how big the dataset should get\n",
    "MODE=\"keywords\" # Alternatively supported: \"headings\"\n",
    "NUMBER_RANKED_DOCUMENTS = 100   # How many documents should be ranked for each query?\n",
    "FILE_PATH = get_path(\"exp/data/008_kategorie_bm25_v2.xlsx\") # The path of where to save it\n",
    "MAX_QUERY_WORDS = 3 # maximum words per query"
   ],
   "id": "1222e96fed375cbc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:24:51.436766Z",
     "start_time": "2024-07-14T19:24:47.220463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    docker compose up -d --build db;\n",
    "    sleep 3;\n",
    "    \"\"\")"
   ],
   "id": "2edbf31f97f6edfd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container project_mse-db-1  Stopping\n",
      " Container project_mse-db-1  Stopped\n",
      " Container project_mse-db-1  Removing\n",
      " Container project_mse-db-1  Removed\n",
      " Network project_mse_default  Removing\n",
      " Network project_mse_default  Removed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [db internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 122B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [db internal] load metadata for docker.io/library/postgres:latest\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [db internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [db 1/2] FROM docker.io/library/postgres:latest\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [db internal] load build context\n",
      "#5 transferring context: 69B done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [db 2/2] COPY dump.sql /docker-entrypoint-initdb.d/\n",
      "#6 CACHED\n",
      "\n",
      "#7 [db] exporting to image\n",
      "#7 exporting layers done\n",
      "#7 writing image sha256:4309df92e6a4a6651b884dd8eea6e932d63c0bf202ba691fa3819b60a26bbc2b done\n",
      "#7 naming to docker.io/library/project_mse-db done\n",
      "#7 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network project_mse_default  Creating\n",
      " Network project_mse_default  Created\n",
      " Container project_mse-db-1  Creating\n",
      " Container project_mse-db-1  Created\n",
      " Container project_mse-db-1  Starting\n",
      " Container project_mse-db-1  Started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:24:52.134900Z",
     "start_time": "2024-07-14T19:24:51.439895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ranker = RankerFlat()\n",
    "documentRepository = ranker.documentRepository\n",
    "tokenizer = ranker.tokenizer"
   ],
   "id": "a472fca372468c70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC: Connected to the db. Now you can go and build the best search engine around!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:24:56.241175Z",
     "start_time": "2024-07-14T19:24:52.136720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_docs = documentRepository.loadAllDocuments()\n",
    "all_encoded_docs = documentRepository.getEncodedTextOfAllDocuments()"
   ],
   "id": "7290f2a6a92e6578",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1306 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-14T19:24:56.242774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_gumbel(eps=1e-20):\n",
    "    shape = (100,)\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def create_own_dataset(all_docs, max_datapoints, mode, n) -> list[QueryResult]:\n",
    "    \"\"\"\n",
    "    created an own dataset based on a list of documents.\n",
    "    \n",
    "    Parameters\n",
    "    all_docs: list of documents\n",
    "    max_datapoints: Maximum number of datapoints to create\n",
    "    mode: The attribute that you want to take for sampling the queries.\n",
    "    n: number of ranked documents per query.\n",
    "    number_query_words: \n",
    "    \"\"\"\n",
    "    assert mode in [\"keywords\", \"headings\"]\n",
    "        \n",
    "    all_attributes = []\n",
    "    results = []\n",
    "    \n",
    "    for doc in all_docs:\n",
    "        attribute = doc.keywords if mode == \"keywords\" else doc.headings\n",
    "        if attribute is None or len(attribute) == 0:\n",
    "            continue\n",
    "        all_attributes.extend(attribute)\n",
    "    all_attributes = list(set(all_attributes))\n",
    "    \n",
    "    for _ in tqdm(range(max_datapoints)):\n",
    "        number_of_words_in_query = random.randint(1, MAX_QUERY_WORDS) # each query can contain [1,MAX_QUERY_WORDS] words\n",
    "        query_list = random.sample(all_attributes, number_of_words_in_query)\n",
    "        query = \" \".join(query_list)\n",
    "        documents_for_query = []\n",
    "        for doc in all_docs:\n",
    "            # if one keyword of the element is matching with the ones from the query\n",
    "            if ((mode == \"keywords\" and bool(set(doc.keywords) & set(query_list))) \n",
    "                    or (mode == \"headings\" and bool(set(doc.headings) & set(query_list)))):\n",
    "                documents_for_query.append(doc)\n",
    "\n",
    "        # here we want to ensure to have n documents to rank. We either cut of if we have more docs or if we dont have enough, we keep sampling from our document list until we have n different documents\n",
    "        if len(documents_for_query) > n:\n",
    "            documents_for_query = documents_for_query[:n]\n",
    "        while len(documents_for_query) < n:\n",
    "            sampled_list = random.sample(all_docs, n - len(documents_for_query))\n",
    "            documents_for_query.extend(sampled_list)\n",
    "            documents_for_query = list(set(documents_for_query))\n",
    "        \n",
    "        # now we will rank each document based on if one of BM25 and normalize them\n",
    "        enc_query = tokenizer.encode(query)\n",
    "        enc_docs = {}\n",
    "        for doc in documents_for_query:\n",
    "            enc_docs[doc.url] = all_encoded_docs[doc.url]\n",
    "        bm25 = ranker.rank_BM25(enc_query, enc_docs)\n",
    "        bm25_ranked = dict(sorted(bm25.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        # we use gumbel noise to rerank the bm25 because we don't fully want to rely on bm25 scores. Finally we use a softmax for our entries to sum up to 1\n",
    "        bm25_tensor = torch.tensor(list(bm25_ranked.values()))\n",
    "        noise = sample_gumbel()\n",
    "        stochastic_rank = torch.softmax(bm25_tensor + noise, dim=-1)\n",
    "        \n",
    "        query_result = QueryResult(query, documents_for_query, stochastic_rank.tolist())\n",
    "        \n",
    "        results.append(query_result)\n",
    "    return results\n",
    "\n",
    "results = create_own_dataset(all_docs, MAX_DATAPOINTS, MODE, NUMBER_RANKED_DOCUMENTS)"
   ],
   "id": "3dbf6e51134bb1e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a89af19201c4bce96dd71c15a719546"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a Dataframe for the results\n",
    "urls = {}\n",
    "for result in results:\n",
    "    urls[result] = [doc.url for doc in result.documents]\n",
    "data = [{'query': query_result.query, 'urls': urls[query_result], 'scores': query_result.scores} for query_result in results]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Size of dataset: {len(df)}\")\n",
    "df.head()"
   ],
   "id": "dee57d2ef8bfbefc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Saving the excel\n",
    "df.to_excel(FILE_PATH, index=False)\n",
    "\n",
    "print(f\"Excel file saved!\")"
   ],
   "id": "4d014a058041fad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From here on, only sanity checks are being done to verify that it also works to load the data back again",
   "id": "a9846501f5492723"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's read the excel file to verify that the dataset can be reconstructed\n",
    "\n",
    "df_reconstruct = pd.read_excel(FILE_PATH)\n",
    "\n",
    "print(f\"Size of reconstructed dataset: {len(df_reconstruct)}, Is test passed: {len(df) == len(df_reconstruct)}\")\n",
    "df_reconstruct.head()"
   ],
   "id": "7592f2d33940c45c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def reconstructDataset(df):\n",
    "    df_list = df.values.tolist()\n",
    "    result = []\n",
    "    for entry in df_list:\n",
    "        documents = []\n",
    "        for url in entry[1]:\n",
    "            documents.append(documentRepository.getDocumentOfUrl(url))\n",
    "        result.append(QueryResult(entry[0], documents, entry[2]))\n",
    "    return result\n",
    "to_query_results = reconstructDataset(df)\n",
    "print(f\"Size of reconstructed dataset: {len(to_query_results)}. Is test passed: {len(df) == len(to_query_results)}\")"
   ],
   "id": "c83b2bad4c56c3d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We want to check if the entries are the same. We cannot make this assert completely but otherwise make weaker statements.",
   "id": "8cb7b8af2f75304f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "(results[0].query == to_query_results[0].query\n",
    "and results[0].documents[0].url == to_query_results[0].documents[0].url\n",
    "and results[0].scores[0] == to_query_results[0].scores[0])"
   ],
   "id": "6959109cc73f42c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
