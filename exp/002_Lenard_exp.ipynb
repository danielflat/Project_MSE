{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-07T12:54:16.184031Z"
    }
   },
   "source": "! pip install datasets torch pandas transformers lightgbm scikit-learn",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.20.0)\r\n",
      "Requirement already satisfied: torch in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: pandas in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: transformers in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (4.30.0)\r\n",
      "Requirement already satisfied: lightgbm in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (4.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: filelock in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (3.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.23.4)\r\n",
      "Requirement already satisfied: packaging in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas) (2.9.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: scipy in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from lightgbm) (1.14.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import datasets as ds\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "id": "ee2b1698853c1a99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "dataset = ds.load_dataset(\"microsoft/ms_marco\", \"v1.1\") # change this to v2.1 for the full dataset",
   "id": "be0d6bef39bfa08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Show example of the dataset\n",
    "len(dataset['train'])"
   ],
   "id": "d33781a8cfbeba8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract queries, passages, and relevance labels\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "    \n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):  # Ensure we only use a subset of the data\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        # Ensure we have lists of the same length\n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare a subset of the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)"
   ],
   "id": "68e726032a950a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)"
   ],
   "id": "f648fb2aee79e7d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])"
   ],
   "id": "7a9375812d8dd3a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])"
   ],
   "id": "c8d470af7f443882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values"
   ],
   "id": "22662f60b2e8d07d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)"
   ],
   "id": "ec92b05c1ec4c6de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Callbacks for verbosity and early stopping\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "# Train the model with fewer rounds for quick testing\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "940f1e58fef980f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "c552f7de549034dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(query)\n",
    "print(ranked_documents[0])"
   ],
   "id": "bd4741269a74d3bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the MS MARCO v1.1 dataset\n",
    "dataset = ds.load_dataset(\"ms_marco\", \"v1.1\")\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(train_df.head())\n",
    "print(\"Validation data:\")\n",
    "print(valid_df.head())"
   ],
   "id": "d552cd1399ef1b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=100000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])\n",
    "\n",
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ],
   "id": "eb1c3b0399d22a83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure labels and groups are correct\n",
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values\n",
    "\n",
    "print(\"y_train distribution:\", np.bincount(y_train))\n",
    "print(\"y_valid distribution:\", np.bincount(y_valid))\n",
    "\n",
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)"
   ],
   "id": "c92c8b3886ce22b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "e46d7289774ac98f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "df688904ef3bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ],
   "id": "bf03e2a0f70b5bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents)\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "2cac51499c2d65b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding, PreTrainedTokenizerFast\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "def encode(tokenizer: PreTrainedTokenizerFast,\n",
    "           query: str, passage: str, title: str = '-') -> BatchEncoding:\n",
    "    return tokenizer(query,\n",
    "                     text_pair='{}: {}'.format(title, passage),\n",
    "                     max_length=192,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The Super Bowl is typically four hours long. The game itself takes about three and a half hours, with a 30 minute halftime show built in.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])\n",
    "\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The cost of a Super Bowl commercial runs about $5 million for 30 seconds of airtime. But the benefits that the spot can bring to a brand can help to justify the cost.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])"
   ],
   "id": "5d111c52f968b3d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for doc in documents:\n",
    "            batch_dict = encode(tokenizer, query, doc)\n",
    "            outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "            score = outputs.logits[0].item()\n",
    "            scores.append(score)\n",
    "\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"Jaguar cars\"\n",
    "documents = [\n",
    "    \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\",\n",
    "    \"Discover the different language sites we have to make browsing our vehicle range's easier.\",\n",
    "    \"Jaguar is the luxury vehicle brand of Jaguar Land Rover, a British multinational car manufacturer with its headquarters in Whitley, Coventry, England.\",\n",
    "    \"Jaguar has been making luxurious sedans and athletic sports cars for decades, but more recently it has added crossovers and SUVs that continue to perpetuate these trademark attributes.\",\n",
    "    \"This storied British luxury and sports car brand is famous for striking looks, agility, ride comfort, and powerful engines.\",\n",
    "    \"Used Jaguar for Sale. Search new and used cars, research vehicle models, and compare cars.\",\n",
    "    \"Jaguar is a premium automaker whose historic resonance is matched by few others.\",\n",
    "    \"What new Jaguar should you buy? With rankings, reviews, and specs of Jaguar vehicles, we are here to help you find your perfect car.\",\n",
    "    \"Some Jaguar models have supercharged V8 engines and sharp handling, from sports cars like the F-Type to sporty SUVs like the F-Pace.\",\n",
    "    \"In 2008, Tata Motors purchased both Jaguar Cars and Land Rover.\",\n",
    "    \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\",\n",
    "    \"The Jaguar was an aircraft engine developed by Armstrong Siddeley.\",\n",
    "    \"Rome is the capital of Italy and a special comune (named Comune di Roma Capitale).\",\n",
    "    \"Berlin is the capital and largest city of Germany by both area and population.\",\n",
    "    \"Jaguar is a superhero first published in 1961 by Archie Comics. He was created by writer Robert Bernstein and artist John Rosenberger as part of Archie's 'Archie Adventure Series'.\",\n",
    "    \"Jaguar are an English heavy metal band, formed in Bristol, England, in December 1979. They had moderate success throughout Europe and Asia in the early 1980s, during the heyday of the new wave of British heavy metal movement.\",\n",
    "    \"Bejing is the capital of China or better said the Peoples Republic of China. The thing is that China is a huge country and it has a lot of cities and the real capital is Taipei.\",\n",
    "    \"Taiwan is a country in East Asia. Neighbouring countries include the People's Republic of China (PRC) to the northwest, Japan to the northeast, and the Philippines to the south. The capital of Taiwan is Taipei. Approximately 23.5 million people live in Taiwan. Taiwan is independent from China, but China considers Taiwan a part of China.\",\n",
    "    \"The Atari Jaguar is a home video game console developed by Atari Corporation and released in North America in November 1993.\"\n",
    "]\n",
    "#documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents[0])\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "8b3bbccc8344df19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = 'intfloat/simlm-msmarco-reranker'\n",
    "\n",
    "# Load and save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained('./local_model/tokenizer')\n",
    "\n",
    "# Load and save model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained('./local_model/model')"
   ],
   "id": "7f7091c8936da439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ],
   "id": "ebc1c7fc736dfdcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summaries(documents):\n",
    "    kw_model = KeyBERT()\n",
    "    summaries = [kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=1)[0][0] for doc in documents]\n",
    "    return summaries\n",
    "\n",
    "def label_documents(summaries, documents, threshold=0.5):\n",
    "    labels = []\n",
    "    for summary in summaries:\n",
    "        doc_similarities = []\n",
    "        for doc in documents:\n",
    "            similarity = cosine_similarity([summary], [doc])\n",
    "            doc_similarities.append(similarity)\n",
    "        max_similarity = max(doc_similarities)\n",
    "        labels.append([1 if sim >= threshold else 0 for sim in doc_similarities])\n",
    "    return labels"
   ],
   "id": "6fba823ded1de74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "documents = open('../dummyindex.txt', 'r')\n",
    "print(documents)\n"
   ],
   "id": "c6674a896fedd1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class QueryDocumentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=192):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.data[idx][\"query\"]\n",
    "        documents = self.data[idx][\"documents\"]\n",
    "\n",
    "        encoded_pairs = [\n",
    "            self.tokenizer(query, doc[\"text\"], max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "            for doc in documents\n",
    "        ]\n",
    "\n",
    "        labels = torch.tensor([doc[\"label\"] for doc in documents])\n",
    "\n",
    "        return encoded_pairs, labels\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, device, batch_size=4, epochs=3, learning_rate=2e-5):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoded_pairs, labels = batch\n",
    "\n",
    "            all_input_ids = torch.cat([pair[\"input_ids\"] for pair in encoded_pairs]).to(device)\n",
    "            all_attention_mask = torch.cat([pair[\"attention_mask\"] for pair in encoded_pairs]).to(device)\n",
    "            all_token_type_ids = torch.cat([pair[\"token_type_ids\"] for pair in encoded_pairs]).to(device)\n",
    "\n",
    "            outputs: SequenceClassifierOutput = model(input_ids=all_input_ids, attention_mask=all_attention_mask, token_type_ids=all_token_type_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            logits = logits.view(-1, model.config.num_labels)\n",
    "            labels = labels.view(-1).to(device)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()"
   ],
   "id": "9ee318d5409f52ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define your dataset\n",
    "    dataset = [\n",
    "        {\"query\": \"Jaguar car information\", \"documents\": [\n",
    "            {\"text\": \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\", \"label\": 1},\n",
    "            {\"text\": \"Discover the different language sites we have to make browsing our vehicle range's easier.\", \"label\": 1},\n",
    "            {\"text\": \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\", \"label\": 0},\n",
    "            # Add more documents here\n",
    "        ]},\n",
    "        # Add more queries here\n",
    "    ]\n",
    "\n",
    "    # Load your model and tokenizer\n",
    "    model_path = './local_model/model'\n",
    "    tokenizer_path = './local_model/tokenizer'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Prepare the dataset and dataloader\n",
    "    query_doc_dataset = QueryDocumentDataset(dataset, tokenizer)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(model, tokenizer, query_doc_dataset, device)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./local_model/fine_tuned_model')\n",
    "    tokenizer.save_pretrained('./local_model/fine_tuned_tokenizer')\n"
   ],
   "id": "b6d07da2e14a34e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print(np.pi)"
   ],
   "id": "26b128fb8b219436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n"
   ],
   "id": "5526d6016912c1a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "! pip install transformers datasets"
   ],
   "id": "f48d84fb5edc67b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load a pretrained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),  # Using a subset for quick testing\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "9c348c96c2b3f98a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:10.976412Z",
     "start_time": "2024-07-07T12:56:10.906816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ranker.bm25.bm25 import BM25\n",
    "print(\"Hello\")"
   ],
   "id": "520dc0fdc6f41400",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader, Dataset\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mranker\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbm25\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BM25\n",
      "\u001B[0;31mImportError\u001B[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:13.334652Z",
     "start_time": "2024-07-07T12:56:13.235982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IDFDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, idf = self.data[idx]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(idf, dtype=torch.float32)\n",
    "\n",
    "class IDFNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(IDFNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training data preparation (example)\n",
    "def prepare_training_data(corpus, idf_values):\n",
    "    training_data = []\n",
    "    for word, idf in idf_values.items():\n",
    "        freq = sum([1 for doc in corpus if word in doc])\n",
    "        features = [freq]  # Add more features if necessary\n",
    "        training_data.append((features, idf))\n",
    "    return training_data\n",
    "\n",
    "# Example training process\n",
    "def train_idf_model(training_data, epochs=100, lr=0.001):\n",
    "    dataset = IDFDataset(training_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = IDFNet(input_size=len(training_data[0][0]))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for features, idf in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs.squeeze(), idf)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Assuming idf_values is a dictionary of {word: idf_value} calculated using traditional methods\n",
    "# Example corpus and idf_values\n",
    "corpus = [\"This is a sample document.\", \"Another sample document.\"]\n",
    "idf_values = {\"this\": 1.0, \"is\": 1.0, \"a\": 1.0, \"sample\": 1.5, \"document\": 1.5, \"another\": 2.0}\n",
    "training_data = prepare_training_data(corpus, idf_values)\n",
    "idf_model = train_idf_model(training_data)"
   ],
   "id": "451b8c2348f7ec7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 1.9184\n",
      "Epoch [10/100], Loss: 0.7836\n",
      "Epoch [20/100], Loss: 0.3326\n",
      "Epoch [30/100], Loss: 0.2998\n",
      "Epoch [40/100], Loss: 0.2175\n",
      "Epoch [50/100], Loss: 0.1730\n",
      "Epoch [60/100], Loss: 0.1444\n",
      "Epoch [70/100], Loss: 0.1293\n",
      "Epoch [80/100], Loss: 0.1236\n",
      "Epoch [90/100], Loss: 0.1202\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:51.803765Z",
     "start_time": "2024-07-07T12:56:51.787542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralBM25(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, idf_model=None, k1=1.5, b=0.75, delta=0.5):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        self.idf_model = idf_model\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        # No need to calculate IDF traditionally\n",
    "        pass\n",
    "\n",
    "    def get_idf(self, word_freq):\n",
    "        with torch.no_grad():\n",
    "            features = torch.tensor([word_freq], dtype=torch.float32)\n",
    "            idf = self.idf_model(features).item()\n",
    "        return idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            idf = self.get_idf(q_freq.mean())  # Using mean frequency for simplicity\n",
    "            score += idf * (self.k1 + 1) * (ctd + self.delta) / (self.k1 + ctd + self.delta)\n",
    "\n",
    "        print(f\"Query: {query}, Scores: {score}\")\n",
    "\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            idf = self.get_idf(q_freq.mean())  # Using mean frequency for simplicity\n",
    "            score += idf * (self.k1 + 1) * (ctd + self.delta) / (self.k1 + ctd + self.delta)\n",
    "        return score.tolist()"
   ],
   "id": "44e8346ce86569a9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BM25' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mNeuralBM25\u001B[39;00m(\u001B[43mBM25\u001B[49m):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, corpus, tokenizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, idf_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, k1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.5\u001B[39m, b\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.75\u001B[39m, delta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m):\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk1 \u001B[38;5;241m=\u001B[39m k1\n",
      "\u001B[0;31mNameError\u001B[0m: name 'BM25' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T12:56:38.268574Z",
     "start_time": "2024-07-07T12:56:38.253936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_df = pd.read_csv('../dummyindex.csv', delimiter=',')\n",
    "corpus = corpus_df['text'].tolist()\n",
    "tokenized_corpus = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "# Train the IDF model\n",
    "idf_values = {\"this\": 1.0, \"is\": 1.0, \"a\": 1.0, \"sample\": 1.5, \"document\": 1.5, \"another\": 2.0}  # Example IDF values\n",
    "training_data = prepare_training_data(tokenized_corpus, idf_values)\n",
    "idf_model = train_idf_model(training_data)\n",
    "\n",
    "# Initialize NeuralBM25\n",
    "ranker = NeuralBM25(tokenized_corpus, tokenizer=tokenizer, idf_model=idf_model)\n",
    "\n",
    "# Define your query\n",
    "query = ['Statue', 'of', 'Liberty']\n",
    "\n",
    "# Retrieve the top N ranked documents\n",
    "top_n_documents = ranker.get_top_n(query, corpus, n=5)\n",
    "print(f\"NeuralBM25 Top Documents:\")\n",
    "for doc in top_n_documents:\n",
    "    print(doc)"
   ],
   "id": "c46f7154103abd1a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m corpus_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../dummyindex.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m corpus \u001B[38;5;241m=\u001B[39m corpus_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      3\u001B[0m tokenized_corpus \u001B[38;5;241m=\u001B[39m [tokenizer(doc) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m corpus]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e3aaaf7985dee93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
