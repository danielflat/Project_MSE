{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-27T16:50:28.137389Z",
     "start_time": "2024-06-27T16:50:23.776900Z"
    }
   },
   "source": "! pip install datasets torch pandas transformers lightgbm scikit-learn",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T13:44:34.696787Z",
     "start_time": "2024-06-28T13:44:26.986738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import datasets as ds\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "id": "ee2b1698853c1a99",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:54:57.210893Z",
     "start_time": "2024-06-27T16:54:53.810579Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = ds.load_dataset(\"microsoft/ms_marco\", \"v1.1\") # change this to v2.1 for the full dataset",
   "id": "be0d6bef39bfa08f",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:15.300054Z",
     "start_time": "2024-06-27T16:55:15.296948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show example of the dataset\n",
    "len(dataset['train'])"
   ],
   "id": "d33781a8cfbeba8b",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:35.328098Z",
     "start_time": "2024-06-27T16:55:34.580034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract queries, passages, and relevance labels\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "    \n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):  # Ensure we only use a subset of the data\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        # Ensure we have lists of the same length\n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare a subset of the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)"
   ],
   "id": "68e726032a950a3a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:02:17.130520Z",
     "start_time": "2024-06-27T17:02:14.022728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)"
   ],
   "id": "f648fb2aee79e7d2",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:53.293950Z",
     "start_time": "2024-06-27T16:55:50.096470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])"
   ],
   "id": "7a9375812d8dd3a6",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:54.286141Z",
     "start_time": "2024-06-27T16:55:53.950159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])"
   ],
   "id": "c8d470af7f443882",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:54.921069Z",
     "start_time": "2024-06-27T16:55:54.915881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values"
   ],
   "id": "22662f60b2e8d07d",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:04.652013Z",
     "start_time": "2024-06-27T17:16:04.645714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)"
   ],
   "id": "ec92b05c1ec4c6de",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:23.346502Z",
     "start_time": "2024-06-27T17:16:21.953159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Callbacks for verbosity and early stopping\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "# Train the model with fewer rounds for quick testing\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "940f1e58fef980f7",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:24.642213Z",
     "start_time": "2024-06-27T17:16:24.625085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "c552f7de549034dd",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:25.328732Z",
     "start_time": "2024-06-27T17:16:25.311747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(query)\n",
    "print(ranked_documents[0])"
   ],
   "id": "bd4741269a74d3bb",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:29:59.927924Z",
     "start_time": "2024-06-27T17:29:33.756533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the MS MARCO v1.1 dataset\n",
    "dataset = ds.load_dataset(\"ms_marco\", \"v1.1\")\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(train_df.head())\n",
    "print(\"Validation data:\")\n",
    "print(valid_df.head())"
   ],
   "id": "d552cd1399ef1b48",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-27T17:33:00.052556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=100000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])\n",
    "\n",
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ],
   "id": "eb1c3b0399d22a83",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:43.422989Z",
     "start_time": "2024-06-27T17:30:27.558316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure labels and groups are correct\n",
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values\n",
    "\n",
    "print(\"y_train distribution:\", np.bincount(y_train))\n",
    "print(\"y_valid distribution:\", np.bincount(y_valid))\n",
    "\n",
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)"
   ],
   "id": "c92c8b3886ce22b4",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:47.670933Z",
     "start_time": "2024-06-27T17:30:47.386127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "e46d7289774ac98f",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:53.632636Z",
     "start_time": "2024-06-27T17:30:53.528316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "df688904ef3bbf",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:59.057287Z",
     "start_time": "2024-06-27T17:30:58.985943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ],
   "id": "bf03e2a0f70b5bbf",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:31:11.899020Z",
     "start_time": "2024-06-27T17:31:11.892684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents)\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "2cac51499c2d65b",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T13:35:38.304793Z",
     "start_time": "2024-06-28T13:34:42.889170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding, PreTrainedTokenizerFast\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "def encode(tokenizer: PreTrainedTokenizerFast,\n",
    "           query: str, passage: str, title: str = '-') -> BatchEncoding:\n",
    "    return tokenizer(query,\n",
    "                     text_pair='{}: {}'.format(title, passage),\n",
    "                     max_length=192,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The Super Bowl is typically four hours long. The game itself takes about three and a half hours, with a 30 minute halftime show built in.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])\n",
    "\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The cost of a Super Bowl commercial runs about $5 million for 30 seconds of airtime. But the benefits that the spot can bring to a brand can help to justify the cost.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])"
   ],
   "id": "5d111c52f968b3d1",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:30:46.861802Z",
     "start_time": "2024-06-28T14:30:45.633085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for doc in documents:\n",
    "            batch_dict = encode(tokenizer, query, doc)\n",
    "            outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "            score = outputs.logits[0].item()\n",
    "            scores.append(score)\n",
    "\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"Jaguar cars\"\n",
    "documents = [\n",
    "    \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\",\n",
    "    \"Discover the different language sites we have to make browsing our vehicle range's easier.\",\n",
    "    \"Jaguar is the luxury vehicle brand of Jaguar Land Rover, a British multinational car manufacturer with its headquarters in Whitley, Coventry, England.\",\n",
    "    \"Jaguar has been making luxurious sedans and athletic sports cars for decades, but more recently it has added crossovers and SUVs that continue to perpetuate these trademark attributes.\",\n",
    "    \"This storied British luxury and sports car brand is famous for striking looks, agility, ride comfort, and powerful engines.\",\n",
    "    \"Used Jaguar for Sale. Search new and used cars, research vehicle models, and compare cars.\",\n",
    "    \"Jaguar is a premium automaker whose historic resonance is matched by few others.\",\n",
    "    \"What new Jaguar should you buy? With rankings, reviews, and specs of Jaguar vehicles, we are here to help you find your perfect car.\",\n",
    "    \"Some Jaguar models have supercharged V8 engines and sharp handling, from sports cars like the F-Type to sporty SUVs like the F-Pace.\",\n",
    "    \"In 2008, Tata Motors purchased both Jaguar Cars and Land Rover.\",\n",
    "    \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\",\n",
    "    \"The Jaguar was an aircraft engine developed by Armstrong Siddeley.\",\n",
    "    \"Rome is the capital of Italy and a special comune (named Comune di Roma Capitale).\",\n",
    "    \"Berlin is the capital and largest city of Germany by both area and population.\",\n",
    "    \"Jaguar is a superhero first published in 1961 by Archie Comics. He was created by writer Robert Bernstein and artist John Rosenberger as part of Archie's 'Archie Adventure Series'.\",\n",
    "    \"Jaguar are an English heavy metal band, formed in Bristol, England, in December 1979. They had moderate success throughout Europe and Asia in the early 1980s, during the heyday of the new wave of British heavy metal movement.\",\n",
    "    \"Bejing is the capital of China or better said the Peoples Republic of China. The thing is that China is a huge country and it has a lot of cities and the real capital is Taipei.\",\n",
    "    \"Taiwan is a country in East Asia. Neighbouring countries include the People's Republic of China (PRC) to the northwest, Japan to the northeast, and the Philippines to the south. The capital of Taiwan is Taipei. Approximately 23.5 million people live in Taiwan. Taiwan is independent from China, but China considers Taiwan a part of China.\",\n",
    "    \"The Atari Jaguar is a home video game console developed by Atari Corporation and released in North America in November 1993.\"\n",
    "]\n",
    "#documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents[0])\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "8b3bbccc8344df19",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-01T09:23:29.697709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = 'intfloat/simlm-msmarco-reranker'\n",
    "\n",
    "# Load and save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained('./local_model/tokenizer')\n",
    "\n",
    "# Load and save model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained('./local_model/model')"
   ],
   "id": "7f7091c8936da439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:34:22.361442Z",
     "start_time": "2024-07-01T10:34:22.357802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n"
   ],
   "id": "ebc1c7fc736dfdcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:32:33.465843Z",
     "start_time": "2024-07-01T10:32:33.462177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summaries(documents):\n",
    "    kw_model = KeyBERT()\n",
    "    summaries = [kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=1)[0][0] for doc in documents]\n",
    "    return summaries\n",
    "\n",
    "def label_documents(summaries, documents, threshold=0.5):\n",
    "    labels = []\n",
    "    for summary in summaries:\n",
    "        doc_similarities = []\n",
    "        for doc in documents:\n",
    "            similarity = cosine_similarity([summary], [doc])\n",
    "            doc_similarities.append(similarity)\n",
    "        max_similarity = max(doc_similarities)\n",
    "        labels.append([1 if sim >= threshold else 0 for sim in doc_similarities])\n",
    "    return labels"
   ],
   "id": "6fba823ded1de74",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:34:08.094982Z",
     "start_time": "2024-07-01T10:34:08.085767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = open('../dummyindex.txt', 'r')\n",
    "print(documents)\n"
   ],
   "id": "c6674a896fedd1ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../dummyindex.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:21:24.303098Z",
     "start_time": "2024-07-01T10:21:24.297249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QueryDocumentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=192):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.data[idx][\"query\"]\n",
    "        documents = self.data[idx][\"documents\"]\n",
    "\n",
    "        encoded_pairs = [\n",
    "            self.tokenizer(query, doc[\"text\"], max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "            for doc in documents\n",
    "        ]\n",
    "\n",
    "        labels = torch.tensor([doc[\"label\"] for doc in documents])\n",
    "\n",
    "        return encoded_pairs, labels\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, device, batch_size=4, epochs=3, learning_rate=2e-5):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoded_pairs, labels = batch\n",
    "\n",
    "            all_input_ids = torch.cat([pair[\"input_ids\"] for pair in encoded_pairs]).to(device)\n",
    "            all_attention_mask = torch.cat([pair[\"attention_mask\"] for pair in encoded_pairs]).to(device)\n",
    "            all_token_type_ids = torch.cat([pair[\"token_type_ids\"] for pair in encoded_pairs]).to(device)\n",
    "\n",
    "            outputs: SequenceClassifierOutput = model(input_ids=all_input_ids, attention_mask=all_attention_mask, token_type_ids=all_token_type_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            logits = logits.view(-1, model.config.num_labels)\n",
    "            labels = labels.view(-1).to(device)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()"
   ],
   "id": "9ee318d5409f52ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-01T09:23:12.675763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define your dataset\n",
    "    dataset = [\n",
    "        {\"query\": \"Jaguar car information\", \"documents\": [\n",
    "            {\"text\": \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\", \"label\": 1},\n",
    "            {\"text\": \"Discover the different language sites we have to make browsing our vehicle range's easier.\", \"label\": 1},\n",
    "            {\"text\": \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\", \"label\": 0},\n",
    "            # Add more documents here\n",
    "        ]},\n",
    "        # Add more queries here\n",
    "    ]\n",
    "\n",
    "    # Load your model and tokenizer\n",
    "    model_path = './local_model/model'\n",
    "    tokenizer_path = './local_model/tokenizer'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Prepare the dataset and dataloader\n",
    "    query_doc_dataset = QueryDocumentDataset(dataset, tokenizer)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(model, tokenizer, query_doc_dataset, device)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./local_model/fine_tuned_model')\n",
    "    tokenizer.save_pretrained('./local_model/fine_tuned_tokenizer')\n"
   ],
   "id": "b6d07da2e14a34e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:37:02.885987Z",
     "start_time": "2024-07-01T16:37:02.883571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print(np.pi)"
   ],
   "id": "26b128fb8b219436",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:40:01.434467Z",
     "start_time": "2024-07-01T16:39:55.219668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n"
   ],
   "id": "5526d6016912c1a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:46:07.589714Z",
     "start_time": "2024-07-01T16:46:02.785116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "! pip install transformers datasets"
   ],
   "id": "f48d84fb5edc67b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\r\n",
      "Requirement already satisfied: torch in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: torchvision in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (0.18.1)\r\n",
      "Requirement already satisfied: torchaudio in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: filelock in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torch) (2024.5.0)\r\n",
      "Requirement already satisfied: numpy in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from torchvision) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: transformers in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (4.41.2)\r\n",
      "Requirement already satisfied: datasets in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (2.20.0)\r\n",
      "Requirement already satisfied: filelock in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (0.23.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:56:41.298903Z",
     "start_time": "2024-07-01T16:53:27.467874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load a pretrained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)),  # Using a subset for quick testing\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "9c348c96c2b3f98a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5f22bf80f624ff19ef1d848ee891618"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92fec3d6625d48beab7aa5410ce92034"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3371f247e59c48d8a99aacc2a9ae2e4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lenardrommel/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 40\u001B[0m\n\u001B[1;32m     32\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     33\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     34\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m     35\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_datasets[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshuffle(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000\u001B[39m)),  \u001B[38;5;66;03m# Using a subset for quick testing\u001B[39;00m\n\u001B[1;32m     36\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mtokenized_datasets[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshuffle(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m     37\u001B[0m )\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/trainer.py:1645\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1640\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1642\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1643\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1644\u001B[0m )\n\u001B[0;32m-> 1645\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1646\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1647\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1648\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1649\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/trainer.py:1929\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1926\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1928\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1929\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1931\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1932\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1933\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1934\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1935\u001B[0m ):\n\u001B[1;32m   1936\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1937\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/trainer.py:2750\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2749\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2750\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2753\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/trainer.py:2775\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2773\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2774\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2775\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2776\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2777\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1562\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1554\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1555\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1562\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1563\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1568\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1569\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1570\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1572\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1574\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   1576\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1006\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m   1008\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m-> 1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m   1021\u001B[0m     embedding_output,\n\u001B[1;32m   1022\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1030\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1031\u001B[0m )\n\u001B[1;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:230\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[1;32m    227\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 230\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m token_type_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings(token_type_ids)\n\u001B[1;32m    233\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Project_MSE/lib/python3.11/site-packages/torch/nn/functional.py:2264\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2258\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2259\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2260\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2261\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2262\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2263\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "451b8c2348f7ec7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
