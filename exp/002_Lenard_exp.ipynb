{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-27T16:50:28.137389Z",
     "start_time": "2024-06-27T16:50:23.776900Z"
    }
   },
   "source": "! pip install datasets torch pandas transformers lightgbm scikit-learn",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T13:44:34.696787Z",
     "start_time": "2024-06-28T13:44:26.986738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import datasets as ds\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "id": "ee2b1698853c1a99",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:54:57.210893Z",
     "start_time": "2024-06-27T16:54:53.810579Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = ds.load_dataset(\"microsoft/ms_marco\", \"v1.1\") # change this to v2.1 for the full dataset",
   "id": "be0d6bef39bfa08f",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:15.300054Z",
     "start_time": "2024-06-27T16:55:15.296948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show example of the dataset\n",
    "len(dataset['train'])"
   ],
   "id": "d33781a8cfbeba8b",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:35.328098Z",
     "start_time": "2024-06-27T16:55:34.580034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract queries, passages, and relevance labels\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "    \n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):  # Ensure we only use a subset of the data\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        # Ensure we have lists of the same length\n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare a subset of the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)"
   ],
   "id": "68e726032a950a3a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:02:17.130520Z",
     "start_time": "2024-06-27T17:02:14.022728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)"
   ],
   "id": "f648fb2aee79e7d2",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:53.293950Z",
     "start_time": "2024-06-27T16:55:50.096470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])"
   ],
   "id": "7a9375812d8dd3a6",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:54.286141Z",
     "start_time": "2024-06-27T16:55:53.950159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])"
   ],
   "id": "c8d470af7f443882",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T16:55:54.921069Z",
     "start_time": "2024-06-27T16:55:54.915881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values"
   ],
   "id": "22662f60b2e8d07d",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:04.652013Z",
     "start_time": "2024-06-27T17:16:04.645714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)"
   ],
   "id": "ec92b05c1ec4c6de",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:23.346502Z",
     "start_time": "2024-06-27T17:16:21.953159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Callbacks for verbosity and early stopping\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "# Train the model with fewer rounds for quick testing\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "940f1e58fef980f7",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:24.642213Z",
     "start_time": "2024-06-27T17:16:24.625085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "c552f7de549034dd",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:16:25.328732Z",
     "start_time": "2024-06-27T17:16:25.311747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(query)\n",
    "print(ranked_documents[0])"
   ],
   "id": "bd4741269a74d3bb",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:29:59.927924Z",
     "start_time": "2024-06-27T17:29:33.756533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the MS MARCO v1.1 dataset\n",
    "dataset = ds.load_dataset(\"ms_marco\", \"v1.1\")\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(dataset_split, num_samples=None):\n",
    "    queries = []\n",
    "    passages = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset_split)\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset_split))):\n",
    "        example = dataset_split[i]\n",
    "        query_id = example['query_id']\n",
    "        query = example['query']\n",
    "        passage_texts = example['passages']['passage_text']\n",
    "        is_selecteds = example['passages']['is_selected']\n",
    "        \n",
    "        if len(passage_texts) != len(is_selecteds):\n",
    "            continue\n",
    "        \n",
    "        for passage_text, is_selected in zip(passage_texts, is_selecteds):\n",
    "            queries.append(query)\n",
    "            passages.append(passage_text)\n",
    "            labels.append(is_selected)\n",
    "            query_ids.append(query_id)\n",
    "\n",
    "    return pd.DataFrame({'query_id': query_ids, 'query': queries, 'passage': passages, 'label': labels})\n",
    "\n",
    "# Prepare the train and validation data\n",
    "train_df = prepare_data(dataset['train'], num_samples=10000)\n",
    "valid_df = prepare_data(dataset['validation'], num_samples=1000)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(train_df.head())\n",
    "print(\"Validation data:\")\n",
    "print(valid_df.head())"
   ],
   "id": "d552cd1399ef1b48",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-27T17:33:00.052556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(max_features=100000)\n",
    "all_text = train_df['query'] + \" \" + train_df['passage']\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "X_train_queries = vectorizer.transform(train_df['query'])\n",
    "X_train_passages = vectorizer.transform(train_df['passage'])\n",
    "X_train = np.hstack([X_train_queries.toarray(), X_train_passages.toarray()])\n",
    "\n",
    "X_valid_queries = vectorizer.transform(valid_df['query'])\n",
    "X_valid_passages = vectorizer.transform(valid_df['passage'])\n",
    "X_valid = np.hstack([X_valid_queries.toarray(), X_valid_passages.toarray()])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ],
   "id": "eb1c3b0399d22a83",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:43.422989Z",
     "start_time": "2024-06-27T17:30:27.558316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure labels and groups are correct\n",
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "\n",
    "group_train = train_df.groupby('query_id').size().values\n",
    "group_valid = valid_df.groupby('query_id').size().values\n",
    "\n",
    "print(\"y_train distribution:\", np.bincount(y_train))\n",
    "print(\"y_valid distribution:\", np.bincount(y_valid))\n",
    "\n",
    "# Train the LambdaMART model deterministically\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'ndcg_eval_at': [10],\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.8,       # Randomly select 80% of the data for each iteration\n",
    "    'bagging_freq': 1,             # Perform bagging every iteration\n",
    "    'feature_fraction': 0.8,       # Randomly select 80% of features for each split\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=10),\n",
    "    lgb.log_evaluation(period=1)\n",
    "]\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=1000,\n",
    "                  callbacks=callbacks)"
   ],
   "id": "c92c8b3886ce22b4",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:47.670933Z",
     "start_time": "2024-06-27T17:30:47.386127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "valid_df['pred'] = y_pred\n",
    "grouped_valid = valid_df.groupby('query_id')\n",
    "\n",
    "ndcg_scores = []\n",
    "for name, group in grouped_valid:\n",
    "    true_relevance = group['label'].values\n",
    "    scores = group['pred'].values\n",
    "    ndcg_scores.append(ndcg_score([true_relevance], [scores], k=10))\n",
    "\n",
    "average_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG: {average_ndcg}\")"
   ],
   "id": "e46d7289774ac98f",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:53.632636Z",
     "start_time": "2024-06-27T17:30:53.528316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model.save_model('lambdamart_model.txt')\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ],
   "id": "df688904ef3bbf",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:30:59.057287Z",
     "start_time": "2024-06-27T17:30:58.985943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and the vectorizer\n",
    "model = lgb.Booster(model_file='lambdamart_model.txt')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ],
   "id": "bf03e2a0f70b5bbf",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:31:11.899020Z",
     "start_time": "2024-06-27T17:31:11.892684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    # Transform the query and documents using the vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    doc_vecs = vectorizer.transform(documents)\n",
    "\n",
    "    # Combine the query and document vectors\n",
    "    combined_vecs = np.hstack([np.tile(query_vec.toarray(), (len(documents), 1)), doc_vecs.toarray()])\n",
    "\n",
    "    # Predict scores using the model\n",
    "    scores = model.predict(combined_vecs)\n",
    "\n",
    "    # Rank documents by score\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of Italy?\"\n",
    "documents = [\n",
    "    \"London is the capital of the United Kingdom.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Pjongyang is the capital of North Korea.\",\n",
    "    \"Tokyo is the capital of Japan.\",\n",
    "    \"Beijing is the capital of China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Madrid is the capital of Spain.\",\n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents)\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "2cac51499c2d65b",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T13:35:38.304793Z",
     "start_time": "2024-06-28T13:34:42.889170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding, PreTrainedTokenizerFast\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "def encode(tokenizer: PreTrainedTokenizerFast,\n",
    "           query: str, passage: str, title: str = '-') -> BatchEncoding:\n",
    "    return tokenizer(query,\n",
    "                     text_pair='{}: {}'.format(title, passage),\n",
    "                     max_length=192,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('intfloat/simlm-msmarco-reranker')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The Super Bowl is typically four hours long. The game itself takes about three and a half hours, with a 30 minute halftime show built in.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])\n",
    "\n",
    "    batch_dict = encode(tokenizer, 'how long is super bowl game', 'The cost of a Super Bowl commercial runs about $5 million for 30 seconds of airtime. But the benefits that the spot can bring to a brand can help to justify the cost.')\n",
    "    outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "    print(outputs.logits[0])"
   ],
   "id": "5d111c52f968b3d1",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T14:30:46.861802Z",
     "start_time": "2024-06-28T14:30:45.633085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_documents(query, documents):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for doc in documents:\n",
    "            batch_dict = encode(tokenizer, query, doc)\n",
    "            outputs: SequenceClassifierOutput = model(**batch_dict, return_dict=True)\n",
    "            score = outputs.logits[0].item()\n",
    "            scores.append(score)\n",
    "\n",
    "    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in ranked_docs], scores\n",
    "\n",
    "# Example usage\n",
    "query = \"Jaguar cars\"\n",
    "documents = [\n",
    "    \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\",\n",
    "    \"Discover the different language sites we have to make browsing our vehicle range's easier.\",\n",
    "    \"Jaguar is the luxury vehicle brand of Jaguar Land Rover, a British multinational car manufacturer with its headquarters in Whitley, Coventry, England.\",\n",
    "    \"Jaguar has been making luxurious sedans and athletic sports cars for decades, but more recently it has added crossovers and SUVs that continue to perpetuate these trademark attributes.\",\n",
    "    \"This storied British luxury and sports car brand is famous for striking looks, agility, ride comfort, and powerful engines.\",\n",
    "    \"Used Jaguar for Sale. Search new and used cars, research vehicle models, and compare cars.\",\n",
    "    \"Jaguar is a premium automaker whose historic resonance is matched by few others.\",\n",
    "    \"What new Jaguar should you buy? With rankings, reviews, and specs of Jaguar vehicles, we are here to help you find your perfect car.\",\n",
    "    \"Some Jaguar models have supercharged V8 engines and sharp handling, from sports cars like the F-Type to sporty SUVs like the F-Pace.\",\n",
    "    \"In 2008, Tata Motors purchased both Jaguar Cars and Land Rover.\",\n",
    "    \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\",\n",
    "    \"The Jaguar was an aircraft engine developed by Armstrong Siddeley.\",\n",
    "    \"Rome is the capital of Italy and a special comune (named Comune di Roma Capitale).\",\n",
    "    \"Berlin is the capital and largest city of Germany by both area and population.\",\n",
    "    \"Jaguar is a superhero first published in 1961 by Archie Comics. He was created by writer Robert Bernstein and artist John Rosenberger as part of Archie's 'Archie Adventure Series'.\",\n",
    "    \"Jaguar are an English heavy metal band, formed in Bristol, England, in December 1979. They had moderate success throughout Europe and Asia in the early 1980s, during the heyday of the new wave of British heavy metal movement.\",\n",
    "    \"Bejing is the capital of China or better said the Peoples Republic of China. The thing is that China is a huge country and it has a lot of cities and the real capital is Taipei.\",\n",
    "    \"Taiwan is a country in East Asia. Neighbouring countries include the People's Republic of China (PRC) to the northwest, Japan to the northeast, and the Philippines to the south. The capital of Taiwan is Taipei. Approximately 23.5 million people live in Taiwan. Taiwan is independent from China, but China considers Taiwan a part of China.\",\n",
    "    \"The Atari Jaguar is a home video game console developed by Atari Corporation and released in North America in November 1993.\"\n",
    "]\n",
    "#documents = np.random.choice(documents, len(documents), replace=False)\n",
    "ranked_documents, scores = rank_documents(query, documents)\n",
    "print(\"Query:\", query)\n",
    "print(\"Ranked Documents:\", ranked_documents[0])\n",
    "print(\"Scores:\", scores)"
   ],
   "id": "8b3bbccc8344df19",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-01T09:23:29.697709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define model name\n",
    "model_name = 'intfloat/simlm-msmarco-reranker'\n",
    "\n",
    "# Load and save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained('./local_model/tokenizer')\n",
    "\n",
    "# Load and save model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.save_pretrained('./local_model/model')"
   ],
   "id": "7f7091c8936da439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:34:22.361442Z",
     "start_time": "2024-07-01T10:34:22.357802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n"
   ],
   "id": "ebc1c7fc736dfdcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:32:33.465843Z",
     "start_time": "2024-07-01T10:32:33.462177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summaries(documents):\n",
    "    kw_model = KeyBERT()\n",
    "    summaries = [kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=1)[0][0] for doc in documents]\n",
    "    return summaries\n",
    "\n",
    "def label_documents(summaries, documents, threshold=0.5):\n",
    "    labels = []\n",
    "    for summary in summaries:\n",
    "        doc_similarities = []\n",
    "        for doc in documents:\n",
    "            similarity = cosine_similarity([summary], [doc])\n",
    "            doc_similarities.append(similarity)\n",
    "        max_similarity = max(doc_similarities)\n",
    "        labels.append([1 if sim >= threshold else 0 for sim in doc_similarities])\n",
    "    return labels"
   ],
   "id": "6fba823ded1de74",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:34:08.094982Z",
     "start_time": "2024-07-01T10:34:08.085767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = open('../dummyindex.txt', 'r')\n",
    "print(documents)\n"
   ],
   "id": "c6674a896fedd1ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='../dummyindex.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T10:21:24.303098Z",
     "start_time": "2024-07-01T10:21:24.297249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QueryDocumentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=192):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.data[idx][\"query\"]\n",
    "        documents = self.data[idx][\"documents\"]\n",
    "\n",
    "        encoded_pairs = [\n",
    "            self.tokenizer(query, doc[\"text\"], max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "            for doc in documents\n",
    "        ]\n",
    "\n",
    "        labels = torch.tensor([doc[\"label\"] for doc in documents])\n",
    "\n",
    "        return encoded_pairs, labels\n",
    "\n",
    "def fine_tune_model(model, tokenizer, dataset, device, batch_size=4, epochs=3, learning_rate=2e-5):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            encoded_pairs, labels = batch\n",
    "\n",
    "            all_input_ids = torch.cat([pair[\"input_ids\"] for pair in encoded_pairs]).to(device)\n",
    "            all_attention_mask = torch.cat([pair[\"attention_mask\"] for pair in encoded_pairs]).to(device)\n",
    "            all_token_type_ids = torch.cat([pair[\"token_type_ids\"] for pair in encoded_pairs]).to(device)\n",
    "\n",
    "            outputs: SequenceClassifierOutput = model(input_ids=all_input_ids, attention_mask=all_attention_mask, token_type_ids=all_token_type_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            logits = logits.view(-1, model.config.num_labels)\n",
    "            labels = labels.view(-1).to(device)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()"
   ],
   "id": "9ee318d5409f52ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-01T09:23:12.675763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define your dataset\n",
    "    dataset = [\n",
    "        {\"query\": \"Jaguar car information\", \"documents\": [\n",
    "            {\"text\": \"The official home of Jaguar USA. Explore our luxury sedans, SUVs and sports cars.\", \"label\": 1},\n",
    "            {\"text\": \"Discover the different language sites we have to make browsing our vehicle range's easier.\", \"label\": 1},\n",
    "            {\"text\": \"The jaguar (Panthera onca) is a large felid species and the only living member of the genus Panthera native to the Americas.\", \"label\": 0},\n",
    "            # Add more documents here\n",
    "        ]},\n",
    "        # Add more queries here\n",
    "    ]\n",
    "\n",
    "    # Load your model and tokenizer\n",
    "    model_path = './local_model/model'\n",
    "    tokenizer_path = './local_model/tokenizer'\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Prepare the dataset and dataloader\n",
    "    query_doc_dataset = QueryDocumentDataset(dataset, tokenizer)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(model, tokenizer, query_doc_dataset, device)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./local_model/fine_tuned_model')\n",
    "    tokenizer.save_pretrained('./local_model/fine_tuned_tokenizer')\n"
   ],
   "id": "b6d07da2e14a34e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-01T09:17:51.799026Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "26b128fb8b219436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5526d6016912c1a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
