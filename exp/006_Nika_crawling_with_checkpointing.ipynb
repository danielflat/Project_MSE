{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U jupyter ipywidgets # for tqdm to function properly\n",
    "! pip install openpyxl docker # for saving stuff to Excel files\n",
    "! pip install fast_langdetect\n",
    "! pip uninstall nltk -y\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: you need Docker to be up and running on your machine for this notebook to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "import docker\n",
    "import ssl\n",
    "# otherwise some pages won't be crawled\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # path is broken on my machine, so I leave this here for myself :)\n",
    "# sys.path.append('/Users/veronicasmilga/project_mse')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: settings for this notebook. If you only want to test, but not want to persist sth., set both booleans to False.\n",
    "SAVE_TO_DATABASE = True # If True, saves the crawled documents to the POSTGRESQL database, else not. Condition: you need docker\n",
    "OVERWRITE_DUMP = True # If True, Overwrites the current \"./db/dump.sql\" with the results from this notebook. Condition: you need docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database by exec docker compose in your terminal. This executes a terminal command using Python\n",
    "if SAVE_TO_DATABASE:\n",
    "    print(os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    docker compose up -d --build db;\n",
    "    sleep 5;\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for the database to function properly, please first go to `exp/001_Flat_db_example_connection.ipynb` and complete the steps from there. If you don't want to be saving documents to the database, just comment out the code after _\"# save one crawled page to database\"_ comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialising the database\n",
    "# documentRepository = DocumentRepository()\n",
    "# Initialising the database\n",
    "if SAVE_TO_DATABASE:\n",
    "    documentRepository = DocumentRepository()\n",
    "    documentRepository.deleteAllDocuments()\n",
    "\n",
    "\n",
    "# initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"raw_html\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start crawling from this cell if you have no checkpoint information and want to start from scratch.\n",
    "\n",
    "NB: I silenced the logs by default, now we only see error output from exceptions. To turn detailed logs back on for debug please initialise the Crawler with verbose=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# NB: this datetime will be used in the name of your Excel crawled_data_backup_file\n",
    "# and json current_state_backup_file\n",
    "print(f\"START DATETIME: {now}\")\n",
    "\n",
    "max_pages = 10000\n",
    "# only applies to websites fully about Tuebingen, i.e. [\"tuebingen_focused_pages\"] in frontier\n",
    "max_steps_per_domain_prioritised = 5000\n",
    "# applies to websites NOT about Tuebingen with just one or two pages about Tuebingen,\n",
    "# i.e. [\"general_pages\"] in frontier and most children links\n",
    "max_steps_per_domain_general = 100\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(\n",
    "    frontier,\n",
    "    max_pages, \n",
    "    max_steps_per_domain_general, \n",
    "    max_steps_per_domain_prioritised, \n",
    "    timeout,\n",
    "    # uncomment if you want to see all the logs\n",
    "    # verbose=True\n",
    "    )\n",
    "\n",
    "for (\n",
    "    # crawled info from page\n",
    "    scraped_webpage_info,\n",
    "    # this and further - state info to be saved to checkpoint file\n",
    "    # from which crawler can be initialised later if our crawling process breaks at some point\n",
    "    to_visit_prioritised, # TÃ¼bingen-related sites and their children\n",
    "    to_visit, # general sites / unknown topic and their children\n",
    "    visited_domains, # domains that should NOT be visited anymore because of reaching max_steps_per_domain\n",
    "    visited, # links that were visited already\n",
    "    domain_steps, # dict of steps made for each domain\n",
    "    extra_links, # links that were extracted but belong to a visited domain and will not be visited on this iteration\n",
    "    page_hashes\n",
    "    ) in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    try:\n",
    "        row = [\n",
    "            str(uuid.uuid4()),\n",
    "            scraped_webpage_info[\"url\"],\n",
    "            scraped_webpage_info[\"title\"],\n",
    "            str(scraped_webpage_info[\"headings\"]),\n",
    "            str(scraped_webpage_info[\"raw_html\"]),\n",
    "            scraped_webpage_info[\"page_text\"],\n",
    "            str(scraped_webpage_info[\"keywords\"]),\n",
    "            scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            str(scraped_webpage_info[\"internal_links\"]),\n",
    "            str(scraped_webpage_info[\"external_links\"])\n",
    "        ]\n",
    "        ws.append(row)\n",
    "        wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "    except Exception as e:\n",
    "        # if something went wrong with Excel, try to save to json instead to preserve info\n",
    "        try:\n",
    "            print(f\"Faced error {e} while trying to save page info to Excel. Saving to backup json file instead.\")\n",
    "            json_filename = f\"data/crawled_data_backup_{now}.json\"\n",
    "            if os.path.exists(json_filename):\n",
    "                with open(json_filename, \"r\") as file:\n",
    "                    backup_file_content = json.load(file)\n",
    "                backup_file_content.append(scraped_webpage_info)\n",
    "            else:\n",
    "                backup_file_content = [scraped_webpage_info]\n",
    "\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(backup_file_content, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"All attempts to save data failed. Skipping webpage {scraped_webpage_info['url']}.\")\n",
    "\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        try:\n",
    "            # save one crawled page to database\n",
    "            document = DocumentEntry(\n",
    "                url=scraped_webpage_info[\"url\"],\n",
    "                title=scraped_webpage_info[\"title\"],\n",
    "                headings=scraped_webpage_info[\"headings\"],\n",
    "                page_text=scraped_webpage_info[\"page_text\"], \n",
    "                keywords=scraped_webpage_info[\"keywords\"],\n",
    "                accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "                internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "                external_links=scraped_webpage_info[\"external_links\"],\n",
    "                id=uuid.uuid4()\n",
    "                )\n",
    "            documentRepository.saveDocument(document)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\"\"Failed to save {scraped_webpage_info[\"url\"]} to database: {e}. Skipping the page.\"\"\")\n",
    "\n",
    "    # save crawling state info on every step\n",
    "    # later crawler can be initialised from this file\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised), \n",
    "        \"to_visit\": list(to_visit), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited),\n",
    "        \"domain_steps\": domain_steps,\n",
    "        \"extra_links\": extra_links, \n",
    "        \"page_hashes\": page_hashes\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all is good with database. It should be of roughly the same length as the df / excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocuments = documentRepository.loadAllDocuments()\n",
    "len(allDocuments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last (very important) step -- saving everything we have crawled into the dump file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_DATABASE:\n",
    "    if OVERWRITE_DUMP:\n",
    "        documentRepository.overwrite_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find your crawled data in `db/dump.sql` file. Alternatively, if something went wrong, you may retrieve the lost data from Excel file named `exp/data/current_state_backup_file_{crawling_date_and_time}.json`, where crawling_date_and_time is the point at which you started crawling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling from checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start crawling from this cell if you have checkpoint information and want to initialise your crawler from a given state (to not crawl the links you crawled already and preserve the info about to_visit queue, visited_domains list, etc.).\n",
    "\n",
    "Look for the checkpoint json file in `exp/data/current_state_backup_file_{crawling_date_and_time}.json`, where crawling_date_and_time is the point at which you started crawling previously and from which you want to resume crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"raw_html\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)\n",
    "\n",
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(f\"START DATETIME: {now}\")\n",
    "\n",
    "# provide the path to the checkpoint file here !!!\n",
    "with open(\"data/current_state_backup_file_2024-07-21_01-02-44.json\", \"r\") as f:\n",
    "    crawling_state = json.load(f)\n",
    "\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)\n",
    "\n",
    "max_pages = 10000\n",
    "# only applies to websites fully about Tuebingen, i.e. [\"tuebingen_focused_pages\"] in frontier\n",
    "max_steps_per_domain_prioritised = 5000\n",
    "# applies to websites NOT about Tuebingen with just one or two pages about Tuebingen,\n",
    "# i.e. [\"general_pages\"] in frontier and most children links\n",
    "max_steps_per_domain_general = 100\n",
    "timeout = 10\n",
    "\n",
    "to_visit_list = crawling_state[\"to_visit\"]\n",
    "to_visit_prioritised_list = crawling_state[\"to_visit_prioritised\"]\n",
    "visited_list = crawling_state[\"visited\"]\n",
    "visited_domains_list = crawling_state[\"visited_domains\"]\n",
    "domain_steps = crawling_state[\"domain_steps\"]\n",
    "extra_links = crawling_state[\"extra_links\"]\n",
    "to_visit = deque(to_visit_list)\n",
    "to_visit_prioritised = deque(to_visit_prioritised_list)\n",
    "visited = set(visited_list)\n",
    "visited_domains = set(visited_domains_list)\n",
    "\n",
    "crawler_1 = Crawler(\n",
    "    frontier,\n",
    "    max_pages, \n",
    "    max_steps_per_domain_general, \n",
    "    max_steps_per_domain_prioritised, \n",
    "    timeout,\n",
    "    visited=visited,\n",
    "    to_visit=to_visit,\n",
    "    to_visit_prioritised=to_visit_prioritised,\n",
    "    visited_domains=visited_domains,\n",
    "    domain_steps=domain_steps,\n",
    "    extra_links=extra_links,\n",
    "    page_hashes=page_hashes\n",
    "    # uncomment if you want to see all the logs\n",
    "    # verbose=True\n",
    "    )\n",
    "\n",
    "for (\n",
    "    scraped_webpage_info,\n",
    "    to_visit_prioritised, \n",
    "    to_visit, \n",
    "    visited_domains, \n",
    "    visited, \n",
    "    domain_steps, \n",
    "    extra_links,\n",
    "    page_hashes\n",
    "    ) in tqdm(crawler_1, total=max_pages):\n",
    "    id =  uuid.uuid4()\n",
    "    try:\n",
    "        row = [\n",
    "            str(id),\n",
    "            scraped_webpage_info[\"url\"],\n",
    "            scraped_webpage_info[\"title\"],\n",
    "            str(scraped_webpage_info[\"headings\"]),\n",
    "            str(scraped_webpage_info[\"raw_html\"]),\n",
    "            scraped_webpage_info[\"page_text\"],\n",
    "            str(scraped_webpage_info[\"keywords\"]),\n",
    "            scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            str(scraped_webpage_info[\"internal_links\"]),\n",
    "            str(scraped_webpage_info[\"external_links\"])\n",
    "        ]\n",
    "        ws.append(row)\n",
    "        wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(f\"Faced error {e} while trying to save page info to Excel. Saving to backup json file instead.\")\n",
    "            json_filename = f\"data/crawled_data_backup_{now}.json\"\n",
    "            if os.path.exists(json_filename):\n",
    "                with open(json_filename, \"r\") as file:\n",
    "                    backup_file_content = json.load(file)\n",
    "                backup_file_content.append(scraped_webpage_info)\n",
    "            else:\n",
    "                backup_file_content = [scraped_webpage_info]\n",
    "\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(backup_file_content, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"All attempts to save data failed. Skipping webpage {scraped_webpage_info['url']}.\")\n",
    "\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        try:\n",
    "            document = DocumentEntry(\n",
    "                url=scraped_webpage_info[\"url\"],\n",
    "                title=scraped_webpage_info[\"title\"],\n",
    "                headings=scraped_webpage_info[\"headings\"],\n",
    "                page_text=scraped_webpage_info[\"page_text\"], \n",
    "                keywords=scraped_webpage_info[\"keywords\"],\n",
    "                accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "                internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "                external_links=scraped_webpage_info[\"external_links\"],\n",
    "                id=id\n",
    "                )\n",
    "            documentRepository.saveDocument(document)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\"\"Failed to save {scraped_webpage_info[\"url\"]} to database: {e}. Skipping the page.\"\"\")\n",
    "\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised), \n",
    "        \"to_visit\": list(to_visit), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited),\n",
    "        \"domain_steps\": domain_steps,\n",
    "        \"extra_links\": extra_links,\n",
    "        \"page_hashes\": page_hashes\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all is good with database. It should be of roughly the same length as the df / excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocuments = documentRepository.loadAllDocuments()\n",
    "len(allDocuments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last (very important) step -- saving everything we have crawled into the dump file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_DATABASE:\n",
    "    if OVERWRITE_DUMP:\n",
    "        documentRepository.overwrite_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can find your crawled data in `db/dump.sql` file. Alternatively, if something went wrong, you may retrieve the lost data from Excel file named `exp/data/current_state_backup_file_{crawling_date_and_time}.json`, where crawling_date_and_time is the point at which you started crawling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
