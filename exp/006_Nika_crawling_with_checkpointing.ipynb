{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorry for installs here, I'll move them to poetry later\n",
    "! pip install -U jupyter ipywidgets # for tqdm to function properly\n",
    "! pip install openpyxl docker # for saving stuff to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "import docker\n",
    "\n",
    "# path is broken on my machine, so I leave this here for myself :)\n",
    "sys.path.append('/Users/veronicasmilga/Desktop/Tübingen/MSE/Project_MSE/')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: settings for this notebook. If you only want to test, but not want to persist sth., set both booleans to False.\n",
    "SAVE_TO_DATABASE = True # If True, saves the crawled documents to the POSTGRESQL database, else not. Condition: you need docker\n",
    "OVERWRITE_DUMP = True # If True, Overwrites the current \"./db/dump.sql\" with the results from this notebook. Condition: you need docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database by exec docker compose in your terminal. This executes a terminal command using Python\n",
    "if SAVE_TO_DATABASE:\n",
    "    print(os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    docker compose up -d --build db;\n",
    "    sleep 5;\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for the database to function properly, please first go to `exp/001_Flat_db_example_connection.ipynb` and complete the steps from there. If you don't want to be saving documents to the database, just comment out the code after _\"# save one crawled page to database\"_ comment.\n",
    "\n",
    "Right now I am only saving to Excel files, but we can put the saved info from the Excel table to the database at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC: Connected to the db. Now you can go and build the best search engine around!\n",
      "SC: Deleted all documents.\n"
     ]
    }
   ],
   "source": [
    "# # initialising the database\n",
    "# documentRepository = DocumentRepository()\n",
    "# Initialising the database\n",
    "if SAVE_TO_DATABASE:\n",
    "    documentRepository = DocumentRepository()\n",
    "    # documentRepository.deleteAllDocuments()\n",
    "\n",
    "\n",
    "# initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"raw_html\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start crawling from this cell if you have no checkpoint information and want to start from scratch.\n",
    "\n",
    "NB: I silenced the logs by default, now we only see error output from exceptions. To turn detailed logs back on for debug please initialise the Crawler with verbose=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START DATETIME: 2024-07-11_18-07-18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b46d562bfc46028225764cf5e9f4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawling from the very beginning. Good luck!\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "Failed to fetch robots.txt for www.hih-tuebingen.de: HTTP Error 404: Not Found\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "Failed to fetch robots.txt for www.bccn-tuebingen.de: HTTP Error 404: Not Found\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "Failed to fetch robots.txt for www.unimuseum.uni-tuebingen.de: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>\n",
      "https://www.unimuseum.uni-tuebingen.de/en/: Failed to fetch page: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "SC: Saved document.\n",
      "Saved checkpoint info to data/current_state_backup_file_2024-07-11_18-07-18.json.\n",
      "Reached the maximum number of pages to crawl.\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# NB: this datetime will be used in the name of your Excel crawled_data_backup_file\n",
    "# and json current_state_backup_file\n",
    "print(f\"START DATETIME: {now}\")\n",
    "\n",
    "max_pages = 15\n",
    "# only applies to websites fully about Tuebingen, i.e. [\"tuebingen_focused_pages\"] in frontier\n",
    "max_steps_per_domain_prioritised = 10\n",
    "# applies to websites NOT about Tuebingen with just one or two pages about Tuebingen,\n",
    "# i.e. [\"general_pages\"] in frontier and most children links\n",
    "max_steps_per_domain_general = 5\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(\n",
    "    frontier,\n",
    "    max_pages, \n",
    "    max_steps_per_domain_general, \n",
    "    max_steps_per_domain_prioritised, \n",
    "    timeout,\n",
    "    # uncomment if you want to see all the logs\n",
    "    # verbose=True\n",
    "    )\n",
    "\n",
    "for (\n",
    "    # crawled info from page\n",
    "    scraped_webpage_info,\n",
    "    # this and further - state info to be saved to checkpoint file\n",
    "    # from which crawler can be initialised later if our crawling process \n",
    "    # breaks at some point\n",
    "    to_visit_prioritised, # Tübingen-related sites and their children\n",
    "    to_visit, # general sites / unknown topic and their children\n",
    "    visited_domains, # domains that should NOT be visited anymore because of reaching max_steps_per_domain\n",
    "    visited, # links that were visited already\n",
    "    domain_steps, # dict of steps made for each domain\n",
    "    extra_links # links that were extracted but belong to a visited domain and will not be visited on this iteration\n",
    "    ) in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        str(scraped_webpage_info[\"raw_html\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    try:\n",
    "        ws.append(row)\n",
    "        wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "    except Exception as e:\n",
    "        # if something went wrong with Excel, try to save to json instead to preserve info\n",
    "        try:\n",
    "            print(f\"Faced error {e} while trying to save page info to Excel. Saving to backup json file instead.\")\n",
    "            json_filename = f\"data/crawled_data_backup_{now}.json\"\n",
    "            if os.path.exists(json_filename):\n",
    "                with open(json_filename, \"r\") as file:\n",
    "                    backup_file_content = json.load(file)\n",
    "                backup_file_content.append(scraped_webpage_info)\n",
    "            else:\n",
    "                backup_file_content = [scraped_webpage_info]\n",
    "\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(backup_file_content, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"All attempts to save data failed. Skipping webpage {scraped_webpage_info['url']}.\")\n",
    "\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        # save one crawled page to database\n",
    "        document = DocumentEntry(\n",
    "            url=scraped_webpage_info[\"url\"],\n",
    "            title=scraped_webpage_info[\"title\"],\n",
    "            headings=scraped_webpage_info[\"headings\"],\n",
    "            page_text=scraped_webpage_info[\"page_text\"], \n",
    "            keywords=scraped_webpage_info[\"keywords\"],\n",
    "            accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "            external_links=scraped_webpage_info[\"external_links\"],\n",
    "            id=uuid.uuid4()\n",
    "            )\n",
    "        documentRepository.saveDocument(document)\n",
    "\n",
    "    # save crawling state info on every step\n",
    "    # later crawler can be initialised from this file\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised), \n",
    "        \"to_visit\": list(to_visit), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited),\n",
    "        \"domain_steps\": domain_steps,\n",
    "        \"extra_links\": extra_links\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved checkpoint info to {json_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start crawling from this cell if you have checkpoint information and want to initialise your crawler from a given state (to not crawl the links you crawled already and preserve the info about to_visit queue, visited_domains list, etc.). NB: this time your crawled data will be saved to a new Excel file, so you will have to merge the old one and the new one manually.\n",
    "\n",
    "NB: right now the state of the crawler (according to what I have crawled already) can be found in `exp/data/current_state_backup_file_2024-07-08_09-57-39.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START DATETIME: 2024-07-11_18-08-42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef9d9ac5810412db9828b2b2e8468cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue crawling from a checkpoint. to_visit_prioritised len: 987, to_visit len: 152, visited len: 17, visited_domains len: 0 extra_links len: 0 domain_steps: {'uni-tuebingen.de': 6, 'www.tuebingen.mpg.de': 1, 'www.tuebingen.de': 1, 'tuebingenresearchcampus.com': 1, 'www.hih-tuebingen.de': 1, 'www.bccn-tuebingen.de': 1, 'www.medizin.uni-tuebingen.de': 1, 'www.my-stuwe.de': 1, 'kunsthalle-tuebingen.de': 1, 'www.neurochirurgie-tuebingen.de': 1},\n",
      "For specific values please refer to the backup json file.\n",
      "SC: Saved document.\n",
      "SC: Saved document.\n",
      "SC: Saved document.\n",
      "SC: Saved document.\n",
      "SC: Saved document.\n",
      "Reached the maximum number of pages to crawl.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"raw_html\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)\n",
    "\n",
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(f\"START DATETIME: {now}\")\n",
    "\n",
    "# provide the path to the checkpoint file here !!!\n",
    "with open(\"data/current_state_backup_file_2024-07-11_18-07-18.json\", \"r\") as f:\n",
    "    crawling_state = json.load(f)\n",
    "\n",
    "max_pages = 5\n",
    "max_steps_per_domain_prioritised = 3000 # only applies to websites fully about Tuebingen\n",
    "max_steps_per_domain_general = 3 # only applies to websites NOT about Tuebingen\n",
    "timeout = 10\n",
    "\n",
    "to_visit_list = crawling_state[\"to_visit\"]\n",
    "to_visit_prioritised_list = crawling_state[\"to_visit_prioritised\"]\n",
    "visited_list = crawling_state[\"visited\"]\n",
    "visited_domains_list = crawling_state[\"visited_domains\"]\n",
    "domain_steps = crawling_state[\"domain_steps\"]\n",
    "extra_links = crawling_state[\"extra_links\"]\n",
    "\n",
    "\n",
    "to_visit = deque(to_visit_list)\n",
    "to_visit_prioritised = deque(to_visit_prioritised_list)\n",
    "visited = set(visited_list)\n",
    "visited_domains = set(visited_domains_list)\n",
    "\n",
    "crawler_1 = Crawler(\n",
    "    frontier,\n",
    "    max_pages, \n",
    "    max_steps_per_domain_general, \n",
    "    max_steps_per_domain_prioritised, \n",
    "    timeout,\n",
    "    visited=visited,\n",
    "    to_visit=to_visit,\n",
    "    to_visit_prioritised=to_visit_prioritised,\n",
    "    visited_domains=visited_domains,\n",
    "    domain_steps=domain_steps,\n",
    "    extra_links=extra_links,\n",
    "    # uncomment if you want to see all the logs\n",
    "    # verbose=True\n",
    "    )\n",
    "id =  uuid.uuid4()\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "for (\n",
    "    scraped_webpage_info,\n",
    "    to_visit_prioritised, \n",
    "    to_visit, \n",
    "    visited_domains, \n",
    "    visited, \n",
    "    domain_steps, \n",
    "    extra_links\n",
    "    ) in tqdm(crawler_1, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(id),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        str(scraped_webpage_info[\"raw_html\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    try:\n",
    "        ws.append(row)\n",
    "        wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(f\"Faced error {e} while trying to save page info to Excel. Saving to backup json file instead.\")\n",
    "            json_filename = f\"data/crawled_data_backup_{now}.json\"\n",
    "            if os.path.exists(json_filename):\n",
    "                with open(json_filename, \"r\") as file:\n",
    "                    backup_file_content = json.load(file)\n",
    "                backup_file_content.append(scraped_webpage_info)\n",
    "            else:\n",
    "                backup_file_content = [scraped_webpage_info]\n",
    "\n",
    "            with open(json_filename, \"w\") as f:\n",
    "                json.dump(backup_file_content, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"All attempts to save data failed. Skipping webpage {scraped_webpage_info['url']}.\")\n",
    "\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        # save one crawled page to database\n",
    "        document = DocumentEntry(\n",
    "            url=scraped_webpage_info[\"url\"],\n",
    "            title=scraped_webpage_info[\"title\"],\n",
    "            headings=scraped_webpage_info[\"headings\"],\n",
    "            page_text=scraped_webpage_info[\"page_text\"], \n",
    "            keywords=scraped_webpage_info[\"keywords\"],\n",
    "            accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "            external_links=scraped_webpage_info[\"external_links\"],\n",
    "            id=id\n",
    "            )\n",
    "        documentRepository.saveDocument(document)\n",
    "\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised), \n",
    "        \"to_visit\": list(to_visit), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited),\n",
    "        \"domain_steps\": domain_steps,\n",
    "        \"extra_links\": extra_links\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you just want to take a look at the data, go to `exp/data` and find the Excel file with timestamp corresponding to the time you ran the Crawler :)\n",
    "\n",
    "Right now, the most recent file is `data/crawled_data_backup_2024-07-08_09-57-39_CONCAT.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>headings</th>\n",
       "      <th>raw_html</th>\n",
       "      <th>page_text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>accessed_timestamp</th>\n",
       "      <th>internal_links</th>\n",
       "      <th>external_links</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59971bc0-c414-428d-828c-249487b49a27</th>\n",
       "      <td>https://uni-tuebingen.de/en/</td>\n",
       "      <td>Home | University of Tübingen</td>\n",
       "      <td>['Studying at the University of Tübingen: Degr...</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html dir=\"ltr\" lang=\"en-GB...</td>\n",
       "      <td>Home | University of Tübingen Skip to main nav...</td>\n",
       "      <td>['podcasts', 'fördermöglichkeiten', 'professor...</td>\n",
       "      <td>2024-07-08 04:01:29.810</td>\n",
       "      <td>['https://uni-tuebingen.de/en/uni-a-z/', 'http...</td>\n",
       "      <td>['https://alma.uni-tuebingen.de/alma/pages/cs/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               url  \\\n",
       "id                                                                   \n",
       "59971bc0-c414-428d-828c-249487b49a27  https://uni-tuebingen.de/en/   \n",
       "\n",
       "                                                              title  \\\n",
       "id                                                                    \n",
       "59971bc0-c414-428d-828c-249487b49a27  Home | University of Tübingen   \n",
       "\n",
       "                                                                               headings  \\\n",
       "id                                                                                        \n",
       "59971bc0-c414-428d-828c-249487b49a27  ['Studying at the University of Tübingen: Degr...   \n",
       "\n",
       "                                                                               raw_html  \\\n",
       "id                                                                                        \n",
       "59971bc0-c414-428d-828c-249487b49a27  b'<!DOCTYPE html>\\n<html dir=\"ltr\" lang=\"en-GB...   \n",
       "\n",
       "                                                                              page_text  \\\n",
       "id                                                                                        \n",
       "59971bc0-c414-428d-828c-249487b49a27  Home | University of Tübingen Skip to main nav...   \n",
       "\n",
       "                                                                               keywords  \\\n",
       "id                                                                                        \n",
       "59971bc0-c414-428d-828c-249487b49a27  ['podcasts', 'fördermöglichkeiten', 'professor...   \n",
       "\n",
       "                                          accessed_timestamp  \\\n",
       "id                                                             \n",
       "59971bc0-c414-428d-828c-249487b49a27 2024-07-08 04:01:29.810   \n",
       "\n",
       "                                                                         internal_links  \\\n",
       "id                                                                                        \n",
       "59971bc0-c414-428d-828c-249487b49a27  ['https://uni-tuebingen.de/en/uni-a-z/', 'http...   \n",
       "\n",
       "                                                                         external_links  \n",
       "id                                                                                       \n",
       "59971bc0-c414-428d-828c-249487b49a27  ['https://alma.uni-tuebingen.de/alma/pages/cs/...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/crawled_data_backup_2024-07-08_09-57-39_CONCAT.xlsx', index_col=0)   \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all is good with database. It should be of roughly the same length as the df / excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "allDocuments = documentRepository.loadAllDocuments()\n",
    "print(len(allDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last (very important) step -- saving everything we have crawled into the dump file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TO_DATABASE:\n",
    "    if OVERWRITE_DUMP:\n",
    "        documentRepository.overwrite_dumb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
