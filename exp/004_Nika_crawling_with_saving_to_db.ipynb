{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorry for installs here, I'll move them to poetry later\n",
    "! pip install -U jupyter ipywidgets # for tqdm to function properly\n",
    "! pip install openpyxl # for saving stuff to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "\n",
    "# path is broken on my machine, so I leave this here for myself :)\n",
    "# sys.path.append('/Users/veronicasmilga/Desktop/Tübingen/MSE/Project_MSE/')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for the database to function properly, please first go to `exp/001_Flat_db_example_connection.ipynb` and complete the steps from there. If you don't want to be saving documents to the database, just comment out the code after _\"# save one crawled page to database\"_ comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the database\n",
    "documentRepository = DocumentRepository()\n",
    "\n",
    "# initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d1cc40a18640ea8e32bde89b9e8d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages crawled: 0. Pages left: 1.\n",
      "Crawling: https://uni-tuebingen.de/en/\n",
      "Indexing...\n",
      "Indexed url https://uni-tuebingen.de/en/ with title `Home | University of Tübingen` successfully.\n",
      "Filtered out invalid internal link: tel:+4970712974444\n",
      "Filtered out invalid internal link: javascript:linkTo_UnCryptMailto(%27ocknvq%2CuvwfkgtgpfgpugmtgvctkcvBbx0wpk%5C%2Fvwgdkpigp0fg%27);\n",
      "Filtered out invalid internal link: tel:+497071290\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "max_pages = 1\n",
    "max_steps_per_domain = 10\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(frontier, max_pages, max_steps_per_domain, timeout)\n",
    "\n",
    "for scraped_webpage_info in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    ws.append(row)\n",
    "    wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "\n",
    "    # save one crawled page to database\n",
    "    document = DocumentEntry(\n",
    "        url=scraped_webpage_info[\"url\"],\n",
    "        title=scraped_webpage_info[\"title\"],\n",
    "        headings=scraped_webpage_info[\"headings\"],\n",
    "        page_text=scraped_webpage_info[\"page_text\"], \n",
    "        keywords=scraped_webpage_info[\"keywords\"],\n",
    "        accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "        external_links=scraped_webpage_info[\"external_links\"],\n",
    "        id=uuid.uuid4()\n",
    "        )\n",
    "    documentRepository.saveDocument(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access documents in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC: Deleted all documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "allDocuments = documentRepository.loadAllDocuments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you just want to take a look at the data, go to `exp/data` and find the Excel file with timestamp corresponding to the time you ran the Crawler :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
