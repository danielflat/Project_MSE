{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import sys\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "import os\n",
    "\n",
    "# path is broken on my machine, so I leave this here for myself :)\n",
    "# sys.path.append('/Users/veronicasmilga/Desktop/TÃ¼bingen/MSE/Project_MSE/')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df: settings for this notebook. If you only want to test, but not want to persist sth., set both booleans to False.\n",
    "SAVE_TO_DATABASE = True # If True, saves the crawled documents to the POSTGRESQL database, else not. Condition: you need docker\n",
    "OVERWRITE_DUMP = True # If True, Overwrites the current \"./db/dump.sql\" with the results from this notebook. Condition: you need docker"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If \"SAVE_TO_DATABASE\" is set to True, we have to set up docker before doing our experiment"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Connect to the database by exec docker compose in your terminal. This executes a terminal command using Python\n",
    "if SAVE_TO_DATABASE:\n",
    "    print(os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    docker compose up -d --build db;\n",
    "    \"\"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialising the database\n",
    "if SAVE_TO_DATABASE:\n",
    "    documentRepository = DocumentRepository()\n",
    "    if OVERWRITE_DUMP:  # if you want to persist the results of this experiments to our database, please make sure to delete the old results a priori\n",
    "        documentRepository.deleteAllDocuments() \n",
    "\n",
    "# Initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Just in case you set \"SAVE_TO_DATABASE\" = True and you did not get\n",
    "`SC: Connected to the db. Now you can go and build the best search engine around!`, just **run the cell again**. \n",
    "\n",
    "Otherwise, continue :)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "max_pages = 5000\n",
    "max_steps_per_domain = 10\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(frontier, max_pages, max_steps_per_domain, timeout)\n",
    "\n",
    "for scraped_webpage_info in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to Excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    ws.append(row)\n",
    "    wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        # save one crawled page to database\n",
    "        document = DocumentEntry(\n",
    "            url=scraped_webpage_info[\"url\"],\n",
    "            title=scraped_webpage_info[\"title\"],\n",
    "            headings=scraped_webpage_info[\"headings\"],\n",
    "            page_text=scraped_webpage_info[\"page_text\"], \n",
    "            keywords=scraped_webpage_info[\"keywords\"],\n",
    "            accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "            external_links=scraped_webpage_info[\"external_links\"],\n",
    "            id=uuid.uuid4()\n",
    "            )\n",
    "        documentRepository.saveDocument(document)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the experiment, you can persist your changes and read out your results from the database.\n",
    "If you want to access documents in the database."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if SAVE_TO_DATABASE:\n",
    "    if OVERWRITE_DUMP:\n",
    "        documentRepository.overwrite_dumb()\n",
    "    allDocuments = documentRepository.loadAllDocuments()\n",
    "    print(len(allDocuments))\n",
    "    #print(allDocuments)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df: Finally let's shut down docker if you are running it\n",
    "if SAVE_TO_DATABASE:\n",
    "    print(os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    \"\"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you just want to take a look at the data, go to `exp/data` and find the Excel file with timestamp corresponding to the time you ran the Crawler :)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "str(documentRepository._get_container_id_by_image(\"project_mse-db\"))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
