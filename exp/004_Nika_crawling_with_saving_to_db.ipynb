{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:35:11.864786Z",
     "start_time": "2024-07-06T18:35:02.629318Z"
    }
   },
   "source": [
    "# import sys\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "import os\n",
    "\n",
    "# path is broken on my machine, so I leave this here for myself :)\n",
    "# sys.path.append('/Users/veronicasmilga/Desktop/Tübingen/MSE/Project_MSE/')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielwps/Documents/Private/Github/Project_MSE/venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to /Users/danielwps/Document\n",
      "[nltk_data]     s/Private/Github/Project_MSE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/danielwps/Documents/Pr\n",
      "[nltk_data]     ivate/Github/Project_MSE/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:35:11.872610Z",
     "start_time": "2024-07-06T18:35:11.867996Z"
    }
   },
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:35:11.877796Z",
     "start_time": "2024-07-06T18:35:11.874491Z"
    }
   },
   "cell_type": "code",
   "source": "SAVE_TO_DATABASE = True",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If \"SAVE_TO_DATABASE\" is set to True, we have to setup docker before that"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:35:18.357130Z",
     "start_time": "2024-07-06T18:35:11.879784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Connect to the database by exec docker compose in your terminal. This exec. a terminal command using Python\n",
    "if SAVE_TO_DATABASE:\n",
    "    os.system(\"\"\"\n",
    "    docker compose down;\n",
    "    docker compose up -d --build db;\n",
    "    \"\"\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container project_mse-db-1  Stopping\n",
      " Container project_mse-db-1  Stopped\n",
      " Container project_mse-db-1  Removing\n",
      " Container project_mse-db-1  Removed\n",
      " Network project_mse_default  Removing\n",
      " Network project_mse_default  Removed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 building with \"desktop-linux\" instance using docker driver\n",
      "\n",
      "#1 [db internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 122B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [db internal] load metadata for docker.io/library/postgres:latest\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [db internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [db 1/2] FROM docker.io/library/postgres:latest\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [db internal] load build context\n",
      "#5 transferring context: 30B done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [db 2/2] COPY dump.sql /docker-entrypoint-initdb.d/\n",
      "#6 CACHED\n",
      "\n",
      "#7 [db] exporting to image\n",
      "#7 exporting layers done\n",
      "#7 writing image sha256:3a71a465c2e6efe5bccb23028b9f1eb04b1b898db39add2d7116c1e6338e75ac done\n",
      "#7 naming to docker.io/library/project_mse-db done\n",
      "#7 DONE 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network project_mse_default  Creating\n",
      " Network project_mse_default  Created\n",
      " Container project_mse-db-1  Creating\n",
      " Container project_mse-db-1  Created\n",
      " Container project_mse-db-1  Starting\n",
      " Container project_mse-db-1  Started\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:38:52.975099Z",
     "start_time": "2024-07-06T18:38:52.955354Z"
    }
   },
   "source": [
    "# initialising the database\n",
    "if SAVE_TO_DATABASE:\n",
    "    documentRepository = DocumentRepository()\n",
    "\n",
    "# initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC: Connected to the db. Now you can go and build the best search engine around!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Just in case you set \"SAVE_TO_DATABASE\" = True and you did not get\n",
    "`SC: Connected to the db. Now you can go and build the best search engine around!`, just **run the cell again**. \n",
    "\n",
    "Otherwise, just continue :)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:40:12.452696Z",
     "start_time": "2024-07-06T18:40:09.510579Z"
    }
   },
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "max_pages = 1\n",
    "max_steps_per_domain = 10\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(frontier, max_pages, max_steps_per_domain, timeout)\n",
    "\n",
    "for scraped_webpage_info in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    ws.append(row)\n",
    "    wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "\n",
    "    if SAVE_TO_DATABASE:\n",
    "        # save one crawled page to database\n",
    "        document = DocumentEntry(\n",
    "            url=scraped_webpage_info[\"url\"],\n",
    "            title=scraped_webpage_info[\"title\"],\n",
    "            headings=scraped_webpage_info[\"headings\"],\n",
    "            page_text=scraped_webpage_info[\"page_text\"], \n",
    "            keywords=scraped_webpage_info[\"keywords\"],\n",
    "            accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "            internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "            external_links=scraped_webpage_info[\"external_links\"],\n",
    "            id=uuid.uuid4()\n",
    "            )\n",
    "        documentRepository.saveDocument(document)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "407dc6797b374a0997e3c1f9ea78a128"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages crawled: 0. Pages left: 1.\n",
      "Crawling: https://uni-tuebingen.de/en/\n",
      "Indexing...\n",
      "Indexed url https://uni-tuebingen.de/en/ with title `Home | University of Tübingen` successfully.\n",
      "Filtered out invalid internal link: tel:+4970712974444\n",
      "Filtered out invalid internal link: javascript:linkTo_UnCryptMailto(%27ocknvq%2CuvwfkgtgpfgpugmtgvctkcvBbx0wpk%5C%2Fvwgdkpigp0fg%27);\n",
      "Filtered out invalid internal link: tel:+497071290\n",
      "SC: Saved document.\n",
      "Reached the maximum number of pages to crawl.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "If you want to access documents in the database. **CAUTION: LONG OUTPUT**. After we will shut down docker again"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:35:25.768422Z",
     "start_time": "2024-07-06T18:35:25.768262Z"
    }
   },
   "source": [
    "if SAVE_TO_DATABASE:\n",
    "    allDocuments = documentRepository.loadAllDocuments()\n",
    "    print(allDocuments)\n",
    "    \n",
    "    # df: Finally let's\n",
    "    os.system(\"\"\"\n",
    "docker compose down;\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you just want to take a look at the data, go to `exp/data` and find the Excel file with timestamp corresponding to the time you ran the Crawler :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
