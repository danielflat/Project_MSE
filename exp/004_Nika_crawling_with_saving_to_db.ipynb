{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorry for installs here, I'll move them to poetry later\n",
    "! pip install -U jupyter ipywidgets # for tqdm to function properly\n",
    "! pip install openpyxl # for saving stuff to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime \n",
    "from openpyxl import Workbook\n",
    "import queue\n",
    "\n",
    "# path is broken on my machine, so I leave this here for myself :)\n",
    "sys.path.append('/Users/veronicasmilga/Desktop/TÃ¼bingen/MSE/Project_MSE/')\n",
    "\n",
    "from db.DocumentEntry import DocumentEntry\n",
    "from db.DocumentRepository import DocumentRepository\n",
    "from data_retrieval.Crawler import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontier now is in a separate file\n",
    "with open(\"../frontier.json\", \"r\") as file:\n",
    "    frontier = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for the database to function properly, please first go to `exp/001_Flat_db_example_connection.ipynb` and complete the steps from there. If you don't want to be saving documents to the database, just comment out the code after _\"# save one crawled page to database\"_ comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialising the database\n",
    "# documentRepository = DocumentRepository()\n",
    "\n",
    "# initialising the Excel backup (if sth goes wrong with the database)\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Crawled Data\"\n",
    "headers = [\"id\", \"url\", \"title\", \"headings\", \"page_text\", \"keywords\", \"accessed_timestamp\", \"internal_links\", \"external_links\"]\n",
    "ws.append(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "max_pages = 5\n",
    "max_steps_per_domain = 3 # only applies to websites NOT about Tuebingen\n",
    "timeout = 10\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(frontier, max_pages, max_steps_per_domain, timeout)\n",
    "\n",
    "for scraped_webpage_info, to_visit_prioritised, to_visit, visited_domains, visited in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    ws.append(row)\n",
    "    wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "\n",
    "    # # save one crawled page to database\n",
    "    # document = DocumentEntry(\n",
    "    #     url=scraped_webpage_info[\"url\"],\n",
    "    #     title=scraped_webpage_info[\"title\"],\n",
    "    #     headings=scraped_webpage_info[\"headings\"],\n",
    "    #     page_text=scraped_webpage_info[\"page_text\"], \n",
    "    #     keywords=scraped_webpage_info[\"keywords\"],\n",
    "    #     accessed_timestamp=scraped_webpage_info[\"accessed_timestamp\"],\n",
    "    #     internal_links=scraped_webpage_info[\"internal_links\"],\n",
    "    #     external_links=scraped_webpage_info[\"external_links\"],\n",
    "    #     id=uuid.uuid4()\n",
    "    #     )\n",
    "    # documentRepository.saveDocument(document)\n",
    "\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised.queue), \n",
    "        \"to_visit\": list(to_visit.queue), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited)\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved checkpoint info to {json_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "with open(\"data/current_state_backup_file_2024-07-04_21-27-38.json\", \"r\") as f:\n",
    "    crawling_state = json.load(f)\n",
    "\n",
    "max_pages = 5\n",
    "max_steps_per_domain = 3 # only applies to websites NOT about Tuebingen\n",
    "timeout = 10\n",
    "\n",
    "to_visit_list = crawling_state[\"to_visit\"]\n",
    "to_visit_prioritised_list = crawling_state[\"to_visit_prioritised\"]\n",
    "visited_list = crawling_state[\"visited\"]\n",
    "visited_domains_list = crawling_state[\"visited_domains\"]\n",
    "\n",
    "to_visit = queue.Queue()\n",
    "for item in to_visit_list:\n",
    "    to_visit.put(item)\n",
    "\n",
    "to_visit_prioritised = queue.Queue()\n",
    "for item in to_visit_prioritised_list:\n",
    "    to_visit_prioritised.put(item)\n",
    "\n",
    "visited = set(visited)\n",
    "visited_domains = set(visited_domains)\n",
    "\n",
    "# Crawler is an iterator now, to handle info generated on-the-fly and save it immediately\n",
    "crawler = Crawler(\n",
    "    frontier,\n",
    "    max_pages, \n",
    "    max_steps_per_domain, \n",
    "    timeout,\n",
    "    to_visit=to_visit,\n",
    "    to_visit_prioritised=to_visit_prioritised,\n",
    "    visited=visited,\n",
    "    visited_domains=visited_domains,\n",
    "    )\n",
    "\n",
    "for scraped_webpage_info, to_visit_prioritised, to_visit, visited_domains, visited in tqdm(crawler, total=max_pages):\n",
    "    # save one crawled page to excel file\n",
    "    row = [\n",
    "        str(uuid.uuid4()),\n",
    "        scraped_webpage_info[\"url\"],\n",
    "        scraped_webpage_info[\"title\"],\n",
    "        str(scraped_webpage_info[\"headings\"]),\n",
    "        scraped_webpage_info[\"page_text\"],\n",
    "        str(scraped_webpage_info[\"keywords\"]),\n",
    "        scraped_webpage_info[\"accessed_timestamp\"],\n",
    "        str(scraped_webpage_info[\"internal_links\"]),\n",
    "        str(scraped_webpage_info[\"external_links\"])\n",
    "    ]\n",
    "    ws.append(row)\n",
    "    wb.save(f\"./data/crawled_data_backup_{now}.xlsx\")\n",
    "\n",
    "    crawling_state = {\n",
    "        \"to_visit_prioritised\": list(to_visit_prioritised.queue), \n",
    "        \"to_visit\": list(to_visit.queue), \n",
    "        \"visited_domains\": list(visited_domains), \n",
    "        \"visited\": list(visited)\n",
    "    }\n",
    "\n",
    "    json_filename = f\"data/current_state_backup_file_{now}.json\"\n",
    "    with open(json_filename, \"w\") as f:\n",
    "        json.dump(crawling_state, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved checkpoint info to {json_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access documents in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allDocuments = documentRepository.loadAllDocuments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you just want to take a look at the data, go to `exp/data` and find the Excel file with timestamp corresponding to the time you ran the Crawler :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-mse-I06HGF-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
