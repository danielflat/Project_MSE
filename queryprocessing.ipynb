{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:41:19.271676Z",
     "start_time": "2024-06-25T19:41:19.012065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import spacy\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ],
   "id": "8ac0ea2665f9f153",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:40:05.832012Z",
     "start_time": "2024-06-25T19:40:03.517310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "# Load pre-trained DistilBART model and tokenizer\n",
    "'''model_name = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "'''\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic has had a significant impact on global economies and public health. \n",
    "Various measures have been implemented to curb the spread of the virus, including lockdowns, \n",
    "social distancing, and vaccination campaigns. These measures have proven effective in reducing \n",
    "the transmission rate and mitigating the effects of the pandemic.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and summarize\n",
    "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=50, min_length=5, length_penalty=10.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)\n",
    "print(len(summary) / len(text), len(summary), len(text))\n",
    "print(\"Time taken:\", time.time() - start)"
   ],
   "id": "bb1793183c0098b0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the COVID-19 pandemic has had a significant impact on global economies and public health. a number of measures have been implemented to curb the spread of the virus.\n",
      "0.47413793103448276 165 348\n",
      "Time taken: 2.3114230632781982\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:40:19.359547Z",
     "start_time": "2024-06-25T19:40:18.697001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic has had a significant impact on global economies and public health. \n",
    "Various measures have been implemented to curb the spread of the virus, including lockdowns, \n",
    "social distancing, and vaccination campaigns. These measures have proven effective in reducing \n",
    "the transmission rate and mitigating the effects of the pandemic.\n",
    "\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract keywords (named entities)\n",
    "keywords = [ent.text for ent in doc.ents]\n",
    "print(\"Keywords:\", keywords)"
   ],
   "id": "32b26cb4ad8ecc6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['COVID-19']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:21:34.777731Z",
     "start_time": "2024-06-25T20:21:33.204910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
   ],
   "id": "f7ad0e85180dc14b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:21:36.739025Z",
     "start_time": "2024-06-25T20:21:36.519794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic has had a significant impact on global economies and public health. \n",
    "Various measures have been implemented to curb the spread of the virus, including lockdowns, \n",
    "social distancing, and vaccination campaigns. These measures have proven effective in reducing \n",
    "the transmission rate and mitigating the effects of the pandemic.\n",
    "\"\"\"\n",
    "\n",
    "# Perform NER\n",
    "ner_results = nlp(text)\n",
    "\n",
    "# Extract and print keywords\n",
    "keywords = [result['word'] for result in ner_results]\n",
    "print(\"Keywords:\", keywords)"
   ],
   "id": "21d06dbf06d637c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['CO', '##VI', '##D', '-', '19']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:35:55.124602Z",
     "start_time": "2024-06-25T20:35:53.715081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keybert import KeyBERT\n",
    "import spacy\n",
    "kw_model = KeyBERT()"
   ],
   "id": "a82c5ec8a1bc51ef",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:34:29.218901Z",
     "start_time": "2024-06-25T20:34:29.153806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample text\n",
    "#text = \n",
    "\"\"\"\n",
    "The COVID-19 pandemic has had a significant impact on global economies and public health. \n",
    "Various measures have been implemented to curb the spread of the virus, including lockdowns, \n",
    "social distancing, and vaccination campaigns. These measures have proven effective in reducing \n",
    "the transmission rate and mitigating the effects of the pandemic.\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "Hello I am an old grandpa who is looking for a job. I have a lot of experience in the field of something that is really important.\n",
    "\"\"\"\n",
    "\n",
    "# Extract keywords\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english')\n",
    "print(text)\n",
    "print(\"Keywords:\", [kw[0] for kw in keywords])\n",
    "\n",
    "# Additional simple keyword extraction based on POS tagging\n",
    "for token in doc:\n",
    "    if token.pos_ in ['NOUN', 'PROPN'] and token.is_stop is False:\n",
    "        keywords.append(token.text)\n",
    "\n",
    "# Remove duplicates and print\n",
    "keywords = list(set(keywords))\n",
    "print(\"Final Keywords:\", keywords)\n"
   ],
   "id": "1182591a16fb58fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello I am an old grandpa who is looking for a job. I have a lot of experience in the field of something that is really important.\n",
      "\n",
      "Keywords: ['old grandpa', 'grandpa looking', 'looking job', 'grandpa', 'job lot']\n",
      "Final Keywords: ['economies', 'vaccination', ('grandpa', 0.4343), 'distancing', 'transmission', 'COVID-19', 'effects', 'impact', ('job lot', 0.3813), 'spread', 'rate', 'pandemic', 'virus', 'lockdowns', ('looking job', 0.4413), 'health', 'campaigns', ('old grandpa', 0.4931), 'measures', ('grandpa looking', 0.4846)]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:36:09.563614Z",
     "start_time": "2024-06-25T20:36:06.417794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Hello I am an old grandpa who is looking for a job. I have a lot of experience in the field of something carpet manufacturing really important.\n",
    "\"\"\"\n",
    "\n",
    "# Process text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities with SpaCy\n",
    "spacy_keywords = [ent.text for ent in doc.ents]\n",
    "\n",
    "# Extract keywords with KeyBERT\n",
    "keybert_keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n",
    "keybert_keywords = [kw[0] for kw in keybert_keywords]\n",
    "\n",
    "# Combine and deduplicate keywords\n",
    "all_keywords = list(set(spacy_keywords + keybert_keywords))\n",
    "print(\"Final Keywords:\", all_keywords)"
   ],
   "id": "18b4672b2f744d62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Keywords: ['carpet', 'looking job', 'carpet manufacturing', 'old grandpa', 'field carpet']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:49:31.575747Z",
     "start_time": "2024-06-25T20:49:30.209404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "I am relly invested in the stock market, I would like to go to Tübigen today and find something that intests me. Maybe some ice cream.\n",
    "\"\"\"\n",
    "\n",
    "text = \"I would like to go somewhere maybe Tübingen. Lets say can you drive a boat in Tübingen? Maybe on the Neckar river.\"\n",
    "\n",
    "text = \"SUP tübingen neckar\"\n",
    "\n",
    "text = \"where is townhall\"\n",
    "\n",
    "# Process text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities and noun chunks with SpaCy\n",
    "spacy_keywords = [ent.text for ent in doc.ents]\n",
    "\n",
    "# Additionally extract noun chunks to capture relevant phrases\n",
    "noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# Extract keywords with KeyBERT\n",
    "keybert_keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)\n",
    "keybert_keywords = [kw[0] for kw in keybert_keywords]\n",
    "\n",
    "# Combine, deduplicate, and filter keywords\n",
    "all_keywords = set(spacy_keywords + noun_chunks + keybert_keywords)\n",
    "\n",
    "# Further filter keywords to focus on nouns and proper nouns\n",
    "filtered_keywords = [word for word in all_keywords if any(token.pos_ in ['NOUN', 'PROPN'] for token in nlp(word))]\n",
    "\n",
    "# Print the final keywords\n",
    "print(\"Final Keywords:\", filtered_keywords)"
   ],
   "id": "d64a0f8789937b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Keywords: ['townhall']\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T20:39:36.702217Z",
     "start_time": "2024-06-25T20:39:33.783799Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d4e34c3b940ab78b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Keywords: []\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7586f30fd06cfda"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
